{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "#from functools import wraps\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "#from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "# from IPython.display import display\n",
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# torch packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.functional import mse_loss, l1_loss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "from torchvision.transforms import v2\n",
    "#from torchvision.utils import make_grid\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "FILENAME = 'SIC_latent_check_'\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.DEBUG,\n",
    "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#     handlers=[\n",
    "#         logging.FileHandler(\"logs/\"+FILENAME+\"_\" + time.strftime(\"%Y%m%d-%H%M%S\")+ \".txt\"),\n",
    "#         #logging.StreamHandler()  #logs to console as well\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create a logger\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "#import custom modules\n",
    "if '../src/modules/' not in sys.path:\n",
    "    sys.path.append('../src/modules/')\n",
    "\n",
    "# if '../src/modules/' not in sys.path:\n",
    "#     sys.path.append('../src/modules/')\n",
    "\n",
    "from plot_jupyter import contour_compare, contour_data\n",
    "from data_wrangle import get_days_before, get_test_set, window_mean, get_days_after\n",
    "from autoenc_lowres import Encoder, Decoder, MLP_enc_SIC, MLP_dec_SIC\n",
    "from autoenc_highres import Encoder_sharp, Decoder_sharp, Encoder_sharp_1024, Decoder_sharp_1024\n",
    "from sic_data_functions import get_ice_data, thin_data, iiee_calc, normalise_image, get_climate_avg_img\n",
    "\n",
    "batched_iiee = torch.func.vmap(iiee_calc)\n",
    "batched_min = torch.func.vmap(torch.min)\n",
    "batched_max = torch.func.vmap(torch.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed for repeatable results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "gen1 = torch.Generator().manual_seed(0)\n",
    "\n",
    "#imported_enc, imported_dec = Encoder, Decoder  # encoder/decoder to be run\n",
    "\n",
    "imported_enc, imported_dec = Encoder_sharp_1024, Decoder_sharp_1024  # encoder/decoder to be run\n",
    "\n",
    "#imported_enc, imported_dec = Encoder_sharp, Decoder_sharp  # encoder/decoder to be run\n",
    "\n",
    "epochs = 1300 #400\n",
    "\n",
    "# if training needs to be run\n",
    "run_training = True\n",
    "\n",
    "run_sharp = True\n",
    "\n",
    "clamp_yn = True\n",
    "norm_yn = False\n",
    "\n",
    "if not run_sharp:\n",
    "    lowres = 'lowres_'\n",
    "else:\n",
    "    lowres = 'sharp_'\n",
    "\n",
    "if not run_training:\n",
    "    load_ext = True\n",
    "    #model_enc = '../outputs/latent_space_verification/enc_lowres_64_1000_20240806-134826'\n",
    "    #model_dec = '../outputs/latent_space_verification/dec_lowres_64_1000_20240806-134826'\n",
    "else:\n",
    "    load_ext = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic_data, m = get_ice_data()\n",
    "\n",
    "if not run_sharp:\n",
    "    sic_data,_ = thin_data(sic_data,m,3)\n",
    "del m\n",
    "\n",
    "#check data with a plot\n",
    "# fig_1, ax_1 = plt.subplots(figsize=(5,4))\n",
    "# ax_1.contourf(sic_data[30][350], cmap=plt.get_cmap('Blues_r'))\n",
    "# ax_1.set_ylim(314, 0)\n",
    "# fig_1.show()\n",
    "\n",
    "#check thinned data with a plot\n",
    "# plt.contourf(DATA[30][350], cmap=plt.get_cmap('Blues_r'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 32 # year within data set\n",
    "day = 0 # day within year as the starting point \n",
    "window = 1 # window-averaging\n",
    "num_year_data = 32 # number of years worth of data\n",
    "T_train = 365 * num_year_data  # how many days worth of data needed\n",
    "batchSize = 210  #215 #batch size for dataloader will vary PC to PC\n",
    "dim_latent = 64 #2,4,8,16,24,32,\n",
    "T_test = 365*2\n",
    "#have run [64]\n",
    "\n",
    "train_split = int(num_year_data*0.8) * 365\n",
    "val_split = num_year_data *365 - train_split\n",
    "\n",
    "\n",
    "\n",
    "# X0_ = get_days_before(DATA, year, day, T_train+window-1) # get days from DATA\n",
    "# X1_ = get_days_after(DATA, year, day,T_test)\n",
    "\n",
    "# if window > 1:\n",
    "#     X0_ = window_mean(X0_, window = window, t = None) # compute window mean, \n",
    "#     X1_ = get_test_set(DATA, year, day, window, T_test)\n",
    "\n",
    "# X0_.shape, X1_.shape\n",
    "\n",
    "\n",
    "\n",
    "X0_ = get_days_before(sic_data, year, day, T_train+window-1)\n",
    "\n",
    "X0_noise, X0_climate_mean = get_climate_avg_img(torch.Tensor(X0_),year)\n",
    "\n",
    "\n",
    "all_data = X0_noise[:,None,:,:]\n",
    "\n",
    "#mask_tensor = torch.Tensor(np.tile(mask, (X0_.shape[0],1,1,1))).bool()\n",
    "\n",
    "#X1_ = get_days_after(sic_data, year, day,  365*2)\n",
    "\n",
    "#all_test_data = torch.Tensor(X1_)[:,None,:,:]\n",
    "\n",
    "all_data_train, all_data_val = torch.utils.data.random_split(all_data, [train_split, val_split], generator=gen1)\n",
    "\n",
    "#full_all_data = torch.utils.data.ConcatDataset([all_data, v2.functional.vflip(all_data), v2.functional.hflip(all_data)])\n",
    "#full_data_train, full_data_val = torch.utils.data.random_split(full_all_data, [train_split*3, val_split*3], generator=gen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.contourf(X0_noise[-10,...].numpy(), cmap = 'Blues_r')\n",
    "# X0_noise.min(),X0_noise.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test dataloaders\n",
    "\n",
    "train_loader_sic = torch.utils.data.DataLoader(dataset=all_data_train, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "val_loader_sic  = torch.utils.data.DataLoader(dataset=all_data_val, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "# train_loader_sic = torch.utils.data.DataLoader(dataset=all_data, batch_size=batchSize, shuffle=True)\n",
    "# train_loader_sic = torch.utils.data.DataLoader(dataset=full_data_train, batch_size=batchSize, shuffle=True)\n",
    "# val_loader_sic  = torch.utils.data.DataLoader(dataset=full_data_val, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 310, 310]             208\n",
      "            Conv2d-2          [-1, 8, 306, 306]           1,608\n",
      "            Conv2d-3          [-1, 8, 302, 302]           1,608\n",
      "       BatchNorm2d-4          [-1, 8, 302, 302]              16\n",
      "              ReLU-5          [-1, 8, 302, 302]               0\n",
      "            Conv2d-6         [-1, 16, 298, 298]           3,216\n",
      "            Conv2d-7         [-1, 16, 294, 294]           6,416\n",
      "            Conv2d-8         [-1, 16, 290, 290]           6,416\n",
      "         MaxPool2d-9         [-1, 16, 145, 145]               0\n",
      "      BatchNorm2d-10         [-1, 16, 145, 145]              32\n",
      "             ReLU-11         [-1, 16, 145, 145]               0\n",
      "           Conv2d-12           [-1, 32, 74, 74]           4,640\n",
      "           Conv2d-13           [-1, 32, 38, 38]           9,248\n",
      "           Conv2d-14           [-1, 32, 20, 20]           9,248\n",
      "        MaxPool2d-15           [-1, 32, 10, 10]               0\n",
      "      BatchNorm2d-16           [-1, 32, 10, 10]              64\n",
      "             ReLU-17           [-1, 32, 10, 10]               0\n",
      "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
      "           Conv2d-19             [-1, 64, 6, 6]          36,928\n",
      "           Conv2d-20             [-1, 64, 4, 4]          36,928\n",
      "      BatchNorm2d-21             [-1, 64, 4, 4]             128\n",
      "          Flatten-22                 [-1, 1024]               0\n",
      "           Linear-23                   [-1, 64]          65,600\n",
      "          Sigmoid-24                   [-1, 64]               0\n",
      "================================================================\n",
      "Total params: 200,800\n",
      "Trainable params: 200,800\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 69.57\n",
      "Params size (MB): 0.77\n",
      "Estimated Total Size (MB): 70.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "enc = imported_enc(d_latent=dim_latent)\n",
    "dec = imported_dec(d_latent=dim_latent)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    enc.to(\"cuda\")  \n",
    "    dec.to(\"cuda\")  \n",
    "\n",
    "if not run_sharp:\n",
    "    summary(enc, (1,105,105))\n",
    "else:\n",
    "    summary(enc, (1,314,314))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_enc,model_dec,epochs, training_dataloader, val_dataloader, \n",
    "                autograd, enc_loc, dec_loc, trainloss_loc, valloss_loc,     \n",
    "                mse_multp = 1.0, mae_multp = 1.0, iiee_multp = 1.0, clamp_img = True, norm_img = False, return_model = False):\n",
    "    # define loss and parameters\n",
    "    \n",
    "    model_enc.train()\n",
    "    model_dec.train()\n",
    "\n",
    "    optimiser = autograd(itertools.chain(model_enc.parameters(), model_dec.parameters()), lr=0.0001)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    val_loss_check = 1e10\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    print('====Training start====')\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_mseloss, total_maeloss, total_iieeloss = 0.0, 0.0, 0.0\n",
    "\n",
    "        total_val_loss, total_train_loss = 0.0, 0.0\n",
    "        \n",
    "        val_loss_mse, val_loss_mae, val_loss_iiee = 0.0, 0.0, 0.0\n",
    "\n",
    "        for batch_idx, data in enumerate(training_dataloader):\n",
    "            # prepare input data\n",
    "            img = data.to(\"cuda\")\n",
    "            \n",
    "            #img = img*mask_tensor[:img.shape[0],...]\n",
    "            output_en = model_enc(img)\n",
    "            output = model_dec(output_en)\n",
    "\n",
    "            if clamp_img == True:\n",
    "                output_norm = output.clamp(min=-1, max= 1)#*mask_tensor\n",
    "            elif norm_img == True:\n",
    "                output_norm = normalise_image(output)\n",
    "            else: \n",
    "                output_norm = output\n",
    "            \n",
    "            loss_mse = mse_loss(output_norm, img)\n",
    "            \n",
    "            loss_mae = l1_loss(img, output_norm)\n",
    "            \n",
    "            loss_iiee = torch.sum(batched_iiee(output_norm, img))\n",
    "\n",
    "            loss = (mse_multp * loss_mse) + (loss_iiee * iiee_multp) + (mae_multp * loss_mae)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Track this epoch's loss\n",
    "            total_mseloss += loss_mse.item() * data.shape[0] / len(all_data_train)\n",
    "            total_maeloss += loss_mae.item() * data.shape[0] / len(all_data_train)\n",
    "            total_iieeloss += loss_iiee.item() * data.shape[0] / len(all_data_train)\n",
    "            total_train_loss += (loss_mse.item() + loss_mae.item() + loss_iiee.item()) * data.shape[0] / len(all_data_train)\n",
    "\n",
    "        train_loss.append([total_mseloss, total_maeloss ,total_iieeloss, total_train_loss])\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        model_enc.eval()\n",
    "        model_dec.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx_val, data_val in enumerate(val_dataloader):\n",
    "\n",
    "                img_val = data_val.to(\"cuda\")\n",
    "                \n",
    "                output_en_val = model_enc(img_val)\n",
    "                output = model_dec(output_en_val)\n",
    "\n",
    "                if clamp_img == True:\n",
    "                    output_norm = output.clamp(min=-1, max= 1)#*mask_tensor\n",
    "                elif norm_img == True:\n",
    "                    output_norm = normalise_image(output)\n",
    "                else: \n",
    "                    output_norm = output\n",
    "\n",
    "                val_loss_mse += mse_loss(output_norm, img_val).item() * data_val.shape[0] / len(all_data_val)\n",
    "                val_loss_mae += l1_loss(output_norm, img_val).item() * data_val.shape[0] / len(all_data_val)\n",
    "                val_loss_iiee += torch.sum(batched_iiee(output_norm, img_val)).item() * data_val.shape[0] / len(all_data_val)\n",
    "\n",
    "                total_val_loss += (\n",
    "                                    (mse_loss(output_norm, img_val).item() * val_loss_mse) + \n",
    "                                    (torch.sum(batched_iiee(output_norm, img_val)).item()  * iiee_multp) + \n",
    "                                    (torch.sum(l1_loss(output_norm, img_val)).item()  * mae_multp)\n",
    "                                  ) * data_val.shape[0] / len(all_data_val)\n",
    "\n",
    "            val_loss.append([val_loss_mse, val_loss_iiee, total_val_loss])\n",
    "\n",
    "        model_enc.train()\n",
    "        model_dec.train()\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        #check val error and save model\n",
    "        if epoch > 0:\n",
    "            if total_val_loss < val_loss_check:\n",
    "                torch.save(model_enc, '../' + enc_loc +'intertrain/img_avg_enc_'+ lowres + str(dim_latent)+'_'+ str(epoch) +'_optimal_' + time.strftime(\"%Y%m%d\"))\n",
    "                torch.save(model_dec, '../' + dec_loc +'intertrain/img_avg_dec_'+  lowres + str(dim_latent)+'_'+ str(epoch) +'_optimal_' +  time.strftime(\"%Y%m%d\"))\n",
    "                \n",
    "                if os.path.exists('../' + enc_loc +'intertrain/intertrain_model_log.txt') == False:\n",
    "                    open('../' + enc_loc +'intertrain/intertrain_model_log.txt', \"w\").close\n",
    "                    \n",
    "                with open('../' + enc_loc +'intertrain/intertrain_model_log.txt', \"a\") as text_file:\n",
    "                    text_file.write('====> Epoch: {}, Train time: {:.4f}, Val time: {:.4f} \\n'.format(epoch, t1-t0, t2-t1))\n",
    "                    text_file.write('====> Epoch: {} Total loss: {:.4f} MSE Loss: {:.4f} MAE Loss: {:.4f} Val loss: {:.4f} \\n'.format(epoch, total_train_loss, total_mseloss, total_maeloss,total_val_loss))\n",
    "                    text_file.write('../' + enc_loc +'intertrain/img_avg_enc_'+ lowres + str(dim_latent)+'_'+ str(epoch) +'_optimal_' + time.strftime(\"%Y%m%d\") + '\\n')\n",
    "                    text_file.write('../' + dec_loc +'intertrain/img_avg_dec_'+  lowres + str(dim_latent)+'_'+ str(epoch) +'_optimal_' +  time.strftime(\"%Y%m%d\")+'\\n')\n",
    "                    text_file.write('\\n')\n",
    "                \n",
    "                val_loss_check = total_val_loss\n",
    "\n",
    "        if epoch%10==0:  \n",
    "            print('====> Epoch: {}, Train time: {:.4f}, Val time: {:.4f}'.format(epoch, t1-t0, t2-t1))\n",
    "            print('====> Epoch: {} Total loss: {:.4f} MSE Loss: {:.4f} MAE Loss: {:.4f} Val loss: {:.4f}'.format(epoch, total_train_loss, total_mseloss, total_maeloss,total_val_loss))\n",
    "    \n",
    "    print('====Training finish====')\n",
    "\n",
    "    torch.save(model_enc, '../' + enc_loc +'img_avg_enc_' + lowres + str(dim_latent)+'_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    torch.save(model_dec, '../' + dec_loc +'img_avg_dec_' + lowres + str(dim_latent)+'_'+ str(epochs) +'_' +  time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "    train_loss_np = np.array(train_loss)\n",
    "    val_loss_np = np.array(val_loss)\n",
    "\n",
    "    with open('../' + trainloss_loc  + lowres + str(dim_latent)+'_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\n",
    "        pickle.dump(train_loss_np, output)\n",
    "\n",
    "    with open('../' + valloss_loc  + lowres + str(dim_latent)+'_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\n",
    "        pickle.dump(val_loss_np, output)\n",
    "\n",
    "    if return_model:\n",
    "        return model_enc,model_dec, train_loss , val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Training start====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\autoenc1\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\ProgramData\\miniconda3\\envs\\autoenc1\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0, Train time: 28.9178, Val time: 1.9246\n",
      "====> Epoch: 0 Total loss: 15919760.6391 MSE Loss: 0.1114 MAE Loss: 0.2827 Val loss: 15863130.4711\n",
      "====> Epoch: 10, Train time: 330.1532, Val time: 1.9339\n",
      "====> Epoch: 10 Total loss: 1270986.0976 MSE Loss: 0.0120 MAE Loss: 0.0450 Val loss: 1250206.3592\n",
      "====> Epoch: 20, Train time: 631.8370, Val time: 1.9279\n",
      "====> Epoch: 20 Total loss: 1261044.1522 MSE Loss: 0.0115 MAE Loss: 0.0425 Val loss: 1244457.5632\n",
      "====> Epoch: 30, Train time: 931.5460, Val time: 1.9259\n",
      "====> Epoch: 30 Total loss: 1250108.9614 MSE Loss: 0.0112 MAE Loss: 0.0417 Val loss: 1239246.4935\n",
      "====> Epoch: 40, Train time: 1231.5868, Val time: 1.8959\n",
      "====> Epoch: 40 Total loss: 1215020.5805 MSE Loss: 0.0111 MAE Loss: 0.0411 Val loss: 1206302.5066\n",
      "====> Epoch: 50, Train time: 1528.0855, Val time: 1.9037\n",
      "====> Epoch: 50 Total loss: 1150626.6343 MSE Loss: 0.0110 MAE Loss: 0.0408 Val loss: 1142910.7255\n",
      "====> Epoch: 60, Train time: 1824.1872, Val time: 1.9047\n",
      "====> Epoch: 60 Total loss: 1120586.2591 MSE Loss: 0.0110 MAE Loss: 0.0405 Val loss: 1111569.1637\n",
      "====> Epoch: 70, Train time: 2120.5145, Val time: 1.8967\n",
      "====> Epoch: 70 Total loss: 1119608.6780 MSE Loss: 0.0109 MAE Loss: 0.0402 Val loss: 1108983.5470\n",
      "====> Epoch: 80, Train time: 2416.7144, Val time: 1.8887\n",
      "====> Epoch: 80 Total loss: 1118263.4285 MSE Loss: 0.0109 MAE Loss: 0.0401 Val loss: 1108132.0812\n",
      "====> Epoch: 90, Train time: 2713.0384, Val time: 1.8891\n",
      "====> Epoch: 90 Total loss: 1116759.8805 MSE Loss: 0.0108 MAE Loss: 0.0401 Val loss: 1105880.8758\n",
      "====> Epoch: 100, Train time: 3009.2854, Val time: 1.8871\n",
      "====> Epoch: 100 Total loss: 1054709.9062 MSE Loss: 0.0108 MAE Loss: 0.0396 Val loss: 1045428.3683\n",
      "====> Epoch: 110, Train time: 3305.6623, Val time: 1.9027\n",
      "====> Epoch: 110 Total loss: 1013799.8516 MSE Loss: 0.0107 MAE Loss: 0.0392 Val loss: 980404.6419\n",
      "====> Epoch: 120, Train time: 3605.5947, Val time: 1.9456\n",
      "====> Epoch: 120 Total loss: 924230.5780 MSE Loss: 0.0107 MAE Loss: 0.0391 Val loss: 916359.3538\n",
      "====> Epoch: 130, Train time: 3907.3186, Val time: 1.9011\n",
      "====> Epoch: 130 Total loss: 923819.8764 MSE Loss: 0.0106 MAE Loss: 0.0385 Val loss: 914254.0656\n",
      "====> Epoch: 140, Train time: 4207.9176, Val time: 1.9084\n",
      "====> Epoch: 140 Total loss: 923880.8114 MSE Loss: 0.0103 MAE Loss: 0.0378 Val loss: 914569.0925\n",
      "====> Epoch: 150, Train time: 4508.5659, Val time: 1.9210\n",
      "====> Epoch: 150 Total loss: 891940.4367 MSE Loss: 0.0096 MAE Loss: 0.0365 Val loss: 882853.2007\n",
      "====> Epoch: 160, Train time: 4809.3282, Val time: 1.9366\n",
      "====> Epoch: 160 Total loss: 821751.6097 MSE Loss: 0.0090 MAE Loss: 0.0353 Val loss: 812968.0898\n",
      "====> Epoch: 170, Train time: 5113.4730, Val time: 1.9472\n",
      "====> Epoch: 170 Total loss: 777677.2128 MSE Loss: 0.0086 MAE Loss: 0.0344 Val loss: 771066.3355\n",
      "====> Epoch: 180, Train time: 5417.5894, Val time: 1.9608\n",
      "====> Epoch: 180 Total loss: 732372.1391 MSE Loss: 0.0080 MAE Loss: 0.0331 Val loss: 725772.9783\n",
      "====> Epoch: 190, Train time: 5718.9860, Val time: 1.8695\n",
      "====> Epoch: 190 Total loss: 702954.7673 MSE Loss: 0.0069 MAE Loss: 0.0311 Val loss: 702703.3599\n",
      "====> Epoch: 200, Train time: 6015.0208, Val time: 1.8718\n",
      "====> Epoch: 200 Total loss: 682738.0966 MSE Loss: 0.0060 MAE Loss: 0.0292 Val loss: 674844.6181\n",
      "====> Epoch: 210, Train time: 6310.1206, Val time: 1.9277\n",
      "====> Epoch: 210 Total loss: 664330.7946 MSE Loss: 0.0055 MAE Loss: 0.0280 Val loss: 666006.7814\n",
      "====> Epoch: 220, Train time: 6609.8836, Val time: 1.8844\n",
      "====> Epoch: 220 Total loss: 641303.2866 MSE Loss: 0.0050 MAE Loss: 0.0268 Val loss: 634349.1224\n",
      "====> Epoch: 230, Train time: 6905.8375, Val time: 1.8734\n",
      "====> Epoch: 230 Total loss: 626541.0050 MSE Loss: 0.0048 MAE Loss: 0.0260 Val loss: 657647.0264\n",
      "====> Epoch: 240, Train time: 7198.2039, Val time: 1.8597\n",
      "====> Epoch: 240 Total loss: 617439.7057 MSE Loss: 0.0046 MAE Loss: 0.0255 Val loss: 630119.6282\n",
      "====> Epoch: 250, Train time: 7490.5035, Val time: 1.8615\n",
      "====> Epoch: 250 Total loss: 601875.7736 MSE Loss: 0.0043 MAE Loss: 0.0246 Val loss: 597707.4765\n",
      "====> Epoch: 260, Train time: 7782.9015, Val time: 1.8620\n",
      "====> Epoch: 260 Total loss: 593665.9813 MSE Loss: 0.0042 MAE Loss: 0.0242 Val loss: 590978.6134\n",
      "====> Epoch: 270, Train time: 8075.1102, Val time: 1.8640\n",
      "====> Epoch: 270 Total loss: 582454.3275 MSE Loss: 0.0040 MAE Loss: 0.0237 Val loss: 623800.3127\n",
      "====> Epoch: 280, Train time: 8367.2999, Val time: 1.8515\n",
      "====> Epoch: 280 Total loss: 574444.8218 MSE Loss: 0.0039 MAE Loss: 0.0234 Val loss: 582734.4793\n",
      "====> Epoch: 290, Train time: 8662.0561, Val time: 1.9216\n",
      "====> Epoch: 290 Total loss: 561825.3967 MSE Loss: 0.0038 MAE Loss: 0.0226 Val loss: 561059.1457\n",
      "====> Epoch: 300, Train time: 8963.6091, Val time: 1.9095\n",
      "====> Epoch: 300 Total loss: 554702.8045 MSE Loss: 0.0037 MAE Loss: 0.0222 Val loss: 557317.0768\n",
      "====> Epoch: 310, Train time: 9267.6967, Val time: 1.9107\n",
      "====> Epoch: 310 Total loss: 550266.4310 MSE Loss: 0.0036 MAE Loss: 0.0219 Val loss: 574584.3511\n",
      "====> Epoch: 320, Train time: 9569.8472, Val time: 1.9412\n",
      "====> Epoch: 320 Total loss: 541701.6606 MSE Loss: 0.0035 MAE Loss: 0.0215 Val loss: 551064.8576\n",
      "====> Epoch: 330, Train time: 9871.6821, Val time: 1.9017\n",
      "====> Epoch: 330 Total loss: 536199.7381 MSE Loss: 0.0034 MAE Loss: 0.0213 Val loss: 605726.6264\n",
      "====> Epoch: 340, Train time: 10172.4286, Val time: 1.9075\n",
      "====> Epoch: 340 Total loss: 530897.3689 MSE Loss: 0.0033 MAE Loss: 0.0210 Val loss: 585356.4063\n",
      "====> Epoch: 350, Train time: 10472.8590, Val time: 1.9262\n",
      "====> Epoch: 350 Total loss: 528093.2915 MSE Loss: 0.0033 MAE Loss: 0.0208 Val loss: 527520.4180\n",
      "====> Epoch: 360, Train time: 10773.3125, Val time: 1.9233\n",
      "====> Epoch: 360 Total loss: 522786.0686 MSE Loss: 0.0032 MAE Loss: 0.0205 Val loss: 553002.6649\n",
      "====> Epoch: 370, Train time: 11073.8464, Val time: 1.9051\n",
      "====> Epoch: 370 Total loss: 518793.8508 MSE Loss: 0.0031 MAE Loss: 0.0203 Val loss: 547048.3498\n",
      "====> Epoch: 380, Train time: 11374.4067, Val time: 1.9140\n",
      "====> Epoch: 380 Total loss: 451817.3879 MSE Loss: 0.0031 MAE Loss: 0.0199 Val loss: 462960.1710\n",
      "====> Epoch: 390, Train time: 11674.9200, Val time: 1.9175\n",
      "====> Epoch: 390 Total loss: 449135.7616 MSE Loss: 0.0030 MAE Loss: 0.0199 Val loss: 478949.7742\n",
      "====> Epoch: 400, Train time: 11975.5440, Val time: 1.9098\n",
      "====> Epoch: 400 Total loss: 443926.8872 MSE Loss: 0.0030 MAE Loss: 0.0196 Val loss: 445272.1842\n",
      "====> Epoch: 410, Train time: 12276.1034, Val time: 1.9278\n",
      "====> Epoch: 410 Total loss: 441052.7073 MSE Loss: 0.0030 MAE Loss: 0.0194 Val loss: 510616.5970\n",
      "====> Epoch: 420, Train time: 12576.6711, Val time: 1.9167\n",
      "====> Epoch: 420 Total loss: 437965.1199 MSE Loss: 0.0029 MAE Loss: 0.0194 Val loss: 466342.6640\n",
      "====> Epoch: 430, Train time: 12877.2779, Val time: 1.9240\n",
      "====> Epoch: 430 Total loss: 434956.9427 MSE Loss: 0.0029 MAE Loss: 0.0193 Val loss: 453681.3898\n",
      "====> Epoch: 440, Train time: 13178.8174, Val time: 1.9485\n",
      "====> Epoch: 440 Total loss: 433593.7446 MSE Loss: 0.0029 MAE Loss: 0.0191 Val loss: 491630.1852\n",
      "====> Epoch: 450, Train time: 13483.1800, Val time: 1.9424\n",
      "====> Epoch: 450 Total loss: 429372.5252 MSE Loss: 0.0028 MAE Loss: 0.0189 Val loss: 431957.6218\n",
      "====> Epoch: 460, Train time: 13787.5620, Val time: 1.9497\n",
      "====> Epoch: 460 Total loss: 428296.9510 MSE Loss: 0.0028 MAE Loss: 0.0189 Val loss: 435991.2381\n",
      "====> Epoch: 470, Train time: 14092.0696, Val time: 1.9480\n",
      "====> Epoch: 470 Total loss: 424002.6679 MSE Loss: 0.0027 MAE Loss: 0.0186 Val loss: 448963.1016\n",
      "====> Epoch: 480, Train time: 14397.9947, Val time: 1.9746\n",
      "====> Epoch: 480 Total loss: 422247.9967 MSE Loss: 0.0027 MAE Loss: 0.0187 Val loss: 440465.8138\n",
      "====> Epoch: 490, Train time: 14703.6027, Val time: 1.9313\n",
      "====> Epoch: 490 Total loss: 378036.7164 MSE Loss: 0.0027 MAE Loss: 0.0184 Val loss: 425226.3499\n",
      "====> Epoch: 500, Train time: 15006.3936, Val time: 1.9856\n",
      "====> Epoch: 500 Total loss: 355894.5098 MSE Loss: 0.0027 MAE Loss: 0.0183 Val loss: 399115.6091\n",
      "====> Epoch: 510, Train time: 15312.6552, Val time: 1.9312\n",
      "====> Epoch: 510 Total loss: 352432.8708 MSE Loss: 0.0026 MAE Loss: 0.0183 Val loss: 383077.0602\n",
      "====> Epoch: 520, Train time: 15616.5281, Val time: 1.9738\n",
      "====> Epoch: 520 Total loss: 350808.3031 MSE Loss: 0.0026 MAE Loss: 0.0183 Val loss: 376919.9916\n",
      "====> Epoch: 530, Train time: 15921.9264, Val time: 1.9358\n",
      "====> Epoch: 530 Total loss: 349039.2524 MSE Loss: 0.0026 MAE Loss: 0.0181 Val loss: 383150.3203\n",
      "====> Epoch: 540, Train time: 16226.9819, Val time: 1.9842\n",
      "====> Epoch: 540 Total loss: 346777.1061 MSE Loss: 0.0026 MAE Loss: 0.0181 Val loss: 350012.7989\n",
      "====> Epoch: 550, Train time: 16530.9589, Val time: 1.9137\n",
      "====> Epoch: 550 Total loss: 346476.1680 MSE Loss: 0.0026 MAE Loss: 0.0180 Val loss: 352149.4017\n",
      "====> Epoch: 560, Train time: 16833.1214, Val time: 1.9386\n",
      "====> Epoch: 560 Total loss: 344278.2017 MSE Loss: 0.0025 MAE Loss: 0.0178 Val loss: 358894.6621\n",
      "====> Epoch: 570, Train time: 17136.9978, Val time: 1.9491\n",
      "====> Epoch: 570 Total loss: 342540.6493 MSE Loss: 0.0025 MAE Loss: 0.0178 Val loss: 358129.8540\n",
      "====> Epoch: 580, Train time: 17439.2590, Val time: 1.9092\n",
      "====> Epoch: 580 Total loss: 342042.0433 MSE Loss: 0.0025 MAE Loss: 0.0178 Val loss: 368459.4842\n",
      "====> Epoch: 590, Train time: 17741.6793, Val time: 1.9350\n",
      "====> Epoch: 590 Total loss: 342704.4696 MSE Loss: 0.0025 MAE Loss: 0.0177 Val loss: 348375.3331\n",
      "====> Epoch: 600, Train time: 18044.5937, Val time: 1.9255\n",
      "====> Epoch: 600 Total loss: 339665.3538 MSE Loss: 0.0025 MAE Loss: 0.0176 Val loss: 341516.4563\n",
      "====> Epoch: 610, Train time: 18347.1033, Val time: 1.9331\n",
      "====> Epoch: 610 Total loss: 338681.5619 MSE Loss: 0.0025 MAE Loss: 0.0175 Val loss: 360620.1689\n",
      "====> Epoch: 620, Train time: 18648.4896, Val time: 1.9532\n",
      "====> Epoch: 620 Total loss: 337036.8232 MSE Loss: 0.0024 MAE Loss: 0.0174 Val loss: 378000.6625\n",
      "====> Epoch: 630, Train time: 18952.8727, Val time: 1.9610\n",
      "====> Epoch: 630 Total loss: 338204.6457 MSE Loss: 0.0024 MAE Loss: 0.0175 Val loss: 355489.3616\n",
      "====> Epoch: 640, Train time: 19255.4282, Val time: 1.9229\n",
      "====> Epoch: 640 Total loss: 335465.8010 MSE Loss: 0.0024 MAE Loss: 0.0172 Val loss: 350477.7986\n",
      "====> Epoch: 650, Train time: 19556.6856, Val time: 1.9196\n",
      "====> Epoch: 650 Total loss: 332695.8333 MSE Loss: 0.0024 MAE Loss: 0.0172 Val loss: 339145.9900\n",
      "====> Epoch: 660, Train time: 19857.9521, Val time: 1.9524\n",
      "====> Epoch: 660 Total loss: 332961.2793 MSE Loss: 0.0024 MAE Loss: 0.0172 Val loss: 366355.4433\n",
      "====> Epoch: 670, Train time: 20155.7039, Val time: 1.9177\n",
      "====> Epoch: 670 Total loss: 346957.0303 MSE Loss: 0.0026 MAE Loss: 0.0178 Val loss: 346442.2919\n",
      "====> Epoch: 680, Train time: 20453.8758, Val time: 1.8989\n",
      "====> Epoch: 680 Total loss: 331893.3910 MSE Loss: 0.0024 MAE Loss: 0.0171 Val loss: 336897.6337\n",
      "====> Epoch: 690, Train time: 20751.1568, Val time: 1.9094\n",
      "====> Epoch: 690 Total loss: 331630.7329 MSE Loss: 0.0024 MAE Loss: 0.0172 Val loss: 394894.1972\n",
      "====> Epoch: 700, Train time: 21048.3526, Val time: 1.9024\n",
      "====> Epoch: 700 Total loss: 329334.5518 MSE Loss: 0.0023 MAE Loss: 0.0169 Val loss: 379735.0457\n",
      "====> Epoch: 710, Train time: 21349.8203, Val time: 1.9138\n",
      "====> Epoch: 710 Total loss: 328891.7145 MSE Loss: 0.0023 MAE Loss: 0.0168 Val loss: 332139.9622\n",
      "====> Epoch: 720, Train time: 21647.7467, Val time: 1.9085\n",
      "====> Epoch: 720 Total loss: 327045.7211 MSE Loss: 0.0023 MAE Loss: 0.0169 Val loss: 331098.9758\n",
      "====> Epoch: 730, Train time: 21945.8712, Val time: 1.9005\n",
      "====> Epoch: 730 Total loss: 325542.7855 MSE Loss: 0.0023 MAE Loss: 0.0167 Val loss: 342232.5518\n",
      "====> Epoch: 740, Train time: 22244.2545, Val time: 1.9470\n",
      "====> Epoch: 740 Total loss: 326374.6421 MSE Loss: 0.0023 MAE Loss: 0.0168 Val loss: 401838.9923\n",
      "====> Epoch: 750, Train time: 22542.1817, Val time: 1.9135\n",
      "====> Epoch: 750 Total loss: 323793.6113 MSE Loss: 0.0023 MAE Loss: 0.0167 Val loss: 339923.1959\n",
      "====> Epoch: 760, Train time: 22841.7070, Val time: 2.0237\n",
      "====> Epoch: 760 Total loss: 323753.2255 MSE Loss: 0.0023 MAE Loss: 0.0166 Val loss: 340938.7981\n",
      "====> Epoch: 770, Train time: 23140.8545, Val time: 1.8616\n",
      "====> Epoch: 770 Total loss: 323036.6177 MSE Loss: 0.0023 MAE Loss: 0.0166 Val loss: 362021.0727\n",
      "====> Epoch: 780, Train time: 23433.8667, Val time: 1.8837\n",
      "====> Epoch: 780 Total loss: 321719.4647 MSE Loss: 0.0022 MAE Loss: 0.0165 Val loss: 336251.1816\n",
      "====> Epoch: 790, Train time: 23726.9389, Val time: 1.8658\n",
      "====> Epoch: 790 Total loss: 323815.8747 MSE Loss: 0.0023 MAE Loss: 0.0165 Val loss: 339427.1131\n",
      "====> Epoch: 800, Train time: 24019.7978, Val time: 1.8840\n",
      "====> Epoch: 800 Total loss: 320386.1490 MSE Loss: 0.0022 MAE Loss: 0.0164 Val loss: 357988.7714\n",
      "====> Epoch: 810, Train time: 24312.4288, Val time: 1.8611\n",
      "====> Epoch: 810 Total loss: 319924.9238 MSE Loss: 0.0022 MAE Loss: 0.0164 Val loss: 329998.4414\n",
      "====> Epoch: 820, Train time: 24606.0996, Val time: 2.0543\n",
      "====> Epoch: 820 Total loss: 322009.0965 MSE Loss: 0.0022 MAE Loss: 0.0164 Val loss: 343934.9899\n",
      "====> Epoch: 830, Train time: 24902.4859, Val time: 1.8977\n",
      "====> Epoch: 830 Total loss: 319637.0404 MSE Loss: 0.0022 MAE Loss: 0.0163 Val loss: 341268.8530\n",
      "====> Epoch: 840, Train time: 25197.7289, Val time: 1.8836\n",
      "====> Epoch: 840 Total loss: 318422.5066 MSE Loss: 0.0022 MAE Loss: 0.0162 Val loss: 323037.4548\n",
      "====> Epoch: 850, Train time: 25491.0032, Val time: 1.8897\n",
      "====> Epoch: 850 Total loss: 317146.8276 MSE Loss: 0.0022 MAE Loss: 0.0161 Val loss: 350769.9487\n",
      "====> Epoch: 860, Train time: 25785.9344, Val time: 1.8834\n",
      "====> Epoch: 860 Total loss: 317358.6265 MSE Loss: 0.0022 MAE Loss: 0.0161 Val loss: 342356.7978\n",
      "====> Epoch: 870, Train time: 26081.6760, Val time: 1.8945\n",
      "====> Epoch: 870 Total loss: 316169.9054 MSE Loss: 0.0022 MAE Loss: 0.0161 Val loss: 321693.7974\n",
      "====> Epoch: 880, Train time: 26377.1051, Val time: 1.9009\n",
      "====> Epoch: 880 Total loss: 316102.8813 MSE Loss: 0.0022 MAE Loss: 0.0161 Val loss: 340831.2225\n",
      "====> Epoch: 890, Train time: 26672.0043, Val time: 1.8977\n",
      "====> Epoch: 890 Total loss: 315230.9968 MSE Loss: 0.0021 MAE Loss: 0.0160 Val loss: 323581.7560\n",
      "====> Epoch: 900, Train time: 26965.7551, Val time: 1.8705\n",
      "====> Epoch: 900 Total loss: 314608.2374 MSE Loss: 0.0021 MAE Loss: 0.0161 Val loss: 398246.0045\n",
      "====> Epoch: 910, Train time: 27258.4868, Val time: 1.8655\n",
      "====> Epoch: 910 Total loss: 314641.0811 MSE Loss: 0.0021 MAE Loss: 0.0160 Val loss: 344735.5239\n",
      "====> Epoch: 920, Train time: 27559.1789, Val time: 1.9540\n",
      "====> Epoch: 920 Total loss: 313736.7906 MSE Loss: 0.0021 MAE Loss: 0.0159 Val loss: 327635.8247\n",
      "====> Epoch: 930, Train time: 27862.0458, Val time: 1.9494\n",
      "====> Epoch: 930 Total loss: 314940.7146 MSE Loss: 0.0021 MAE Loss: 0.0160 Val loss: 444030.2951\n",
      "====> Epoch: 940, Train time: 28156.4033, Val time: 1.8717\n",
      "====> Epoch: 940 Total loss: 312296.0152 MSE Loss: 0.0021 MAE Loss: 0.0159 Val loss: 316678.8791\n",
      "====> Epoch: 950, Train time: 28450.5069, Val time: 1.9487\n",
      "====> Epoch: 950 Total loss: 313558.4930 MSE Loss: 0.0021 MAE Loss: 0.0158 Val loss: 327201.0575\n",
      "====> Epoch: 960, Train time: 28746.0911, Val time: 1.8809\n",
      "====> Epoch: 960 Total loss: 311965.3384 MSE Loss: 0.0021 MAE Loss: 0.0158 Val loss: 321654.9887\n",
      "====> Epoch: 970, Train time: 29041.1487, Val time: 1.8913\n",
      "====> Epoch: 970 Total loss: 310530.6452 MSE Loss: 0.0021 MAE Loss: 0.0158 Val loss: 343368.0442\n",
      "====> Epoch: 980, Train time: 29338.4891, Val time: 1.9642\n",
      "====> Epoch: 980 Total loss: 310493.3471 MSE Loss: 0.0021 MAE Loss: 0.0157 Val loss: 325328.4821\n",
      "====> Epoch: 990, Train time: 29634.4632, Val time: 1.8730\n",
      "====> Epoch: 990 Total loss: 310082.0665 MSE Loss: 0.0021 MAE Loss: 0.0157 Val loss: 320829.6736\n",
      "====> Epoch: 1000, Train time: 29927.3327, Val time: 1.8724\n",
      "====> Epoch: 1000 Total loss: 309840.0221 MSE Loss: 0.0021 MAE Loss: 0.0157 Val loss: 313741.7008\n",
      "====> Epoch: 1010, Train time: 30220.7164, Val time: 1.8741\n",
      "====> Epoch: 1010 Total loss: 309706.1120 MSE Loss: 0.0021 MAE Loss: 0.0157 Val loss: 324869.4957\n",
      "====> Epoch: 1020, Train time: 30519.4908, Val time: 1.9264\n",
      "====> Epoch: 1020 Total loss: 308612.1787 MSE Loss: 0.0021 MAE Loss: 0.0156 Val loss: 316923.0434\n",
      "====> Epoch: 1030, Train time: 30818.1614, Val time: 1.9103\n",
      "====> Epoch: 1030 Total loss: 308119.5781 MSE Loss: 0.0020 MAE Loss: 0.0155 Val loss: 326771.0714\n",
      "====> Epoch: 1040, Train time: 31112.1870, Val time: 1.8910\n",
      "====> Epoch: 1040 Total loss: 307284.2252 MSE Loss: 0.0020 MAE Loss: 0.0155 Val loss: 313814.5641\n",
      "====> Epoch: 1050, Train time: 31407.8632, Val time: 1.8567\n",
      "====> Epoch: 1050 Total loss: 307248.9808 MSE Loss: 0.0020 MAE Loss: 0.0155 Val loss: 349054.4138\n",
      "====> Epoch: 1060, Train time: 31712.5168, Val time: 2.0667\n",
      "====> Epoch: 1060 Total loss: 307121.4180 MSE Loss: 0.0020 MAE Loss: 0.0155 Val loss: 328378.1947\n",
      "====> Epoch: 1070, Train time: 32022.8217, Val time: 2.0290\n",
      "====> Epoch: 1070 Total loss: 307219.5994 MSE Loss: 0.0020 MAE Loss: 0.0155 Val loss: 315499.1940\n",
      "====> Epoch: 1080, Train time: 32316.1927, Val time: 1.8570\n",
      "====> Epoch: 1080 Total loss: 305857.6174 MSE Loss: 0.0020 MAE Loss: 0.0154 Val loss: 312557.0842\n",
      "====> Epoch: 1090, Train time: 32607.6042, Val time: 1.8475\n",
      "====> Epoch: 1090 Total loss: 306110.3051 MSE Loss: 0.0020 MAE Loss: 0.0154 Val loss: 316519.8787\n",
      "====> Epoch: 1100, Train time: 32899.8270, Val time: 1.8524\n",
      "====> Epoch: 1100 Total loss: 305579.8277 MSE Loss: 0.0020 MAE Loss: 0.0153 Val loss: 335973.0440\n",
      "====> Epoch: 1110, Train time: 33192.1184, Val time: 1.8601\n",
      "====> Epoch: 1110 Total loss: 305607.9160 MSE Loss: 0.0020 MAE Loss: 0.0153 Val loss: 326162.8792\n",
      "====> Epoch: 1120, Train time: 33484.2802, Val time: 1.8426\n",
      "====> Epoch: 1120 Total loss: 304157.4025 MSE Loss: 0.0020 MAE Loss: 0.0153 Val loss: 337870.0855\n",
      "====> Epoch: 1130, Train time: 33776.4957, Val time: 1.8733\n",
      "====> Epoch: 1130 Total loss: 304892.0096 MSE Loss: 0.0020 MAE Loss: 0.0153 Val loss: 320367.7969\n",
      "====> Epoch: 1140, Train time: 34068.8538, Val time: 1.8457\n",
      "====> Epoch: 1140 Total loss: 303890.8956 MSE Loss: 0.0020 MAE Loss: 0.0152 Val loss: 333761.2494\n",
      "====> Epoch: 1150, Train time: 34360.9993, Val time: 1.8547\n",
      "====> Epoch: 1150 Total loss: 303065.1283 MSE Loss: 0.0020 MAE Loss: 0.0151 Val loss: 311929.5361\n",
      "====> Epoch: 1160, Train time: 34653.1455, Val time: 1.8731\n",
      "====> Epoch: 1160 Total loss: 302470.0544 MSE Loss: 0.0020 MAE Loss: 0.0151 Val loss: 337235.6464\n",
      "====> Epoch: 1170, Train time: 34945.7572, Val time: 1.8714\n",
      "====> Epoch: 1170 Total loss: 303555.0391 MSE Loss: 0.0020 MAE Loss: 0.0152 Val loss: 315501.8651\n",
      "====> Epoch: 1180, Train time: 35238.1195, Val time: 1.8734\n",
      "====> Epoch: 1180 Total loss: 302538.3667 MSE Loss: 0.0020 MAE Loss: 0.0151 Val loss: 315884.0429\n",
      "====> Epoch: 1190, Train time: 35530.6125, Val time: 1.8731\n",
      "====> Epoch: 1190 Total loss: 304721.1602 MSE Loss: 0.0020 MAE Loss: 0.0153 Val loss: 328239.1674\n",
      "====> Epoch: 1200, Train time: 35823.0521, Val time: 1.8569\n",
      "====> Epoch: 1200 Total loss: 301702.7320 MSE Loss: 0.0019 MAE Loss: 0.0150 Val loss: 345253.8933\n",
      "====> Epoch: 1210, Train time: 36115.5971, Val time: 1.8631\n",
      "====> Epoch: 1210 Total loss: 303520.0395 MSE Loss: 0.0020 MAE Loss: 0.0151 Val loss: 374131.2511\n",
      "====> Epoch: 1220, Train time: 36408.2121, Val time: 1.8670\n",
      "====> Epoch: 1220 Total loss: 301048.8668 MSE Loss: 0.0019 MAE Loss: 0.0150 Val loss: 327116.7146\n",
      "====> Epoch: 1230, Train time: 36700.6256, Val time: 1.8643\n",
      "====> Epoch: 1230 Total loss: 300833.1095 MSE Loss: 0.0019 MAE Loss: 0.0150 Val loss: 345596.3727\n",
      "====> Epoch: 1240, Train time: 36993.2234, Val time: 1.8601\n",
      "====> Epoch: 1240 Total loss: 300829.8717 MSE Loss: 0.0019 MAE Loss: 0.0149 Val loss: 307575.2893\n",
      "====> Epoch: 1250, Train time: 37289.0890, Val time: 1.9147\n",
      "====> Epoch: 1250 Total loss: 300660.3511 MSE Loss: 0.0019 MAE Loss: 0.0149 Val loss: 309819.3989\n",
      "====> Epoch: 1260, Train time: 37590.3069, Val time: 1.9208\n",
      "====> Epoch: 1260 Total loss: 302274.4252 MSE Loss: 0.0019 MAE Loss: 0.0151 Val loss: 309122.4537\n",
      "====> Epoch: 1270, Train time: 37891.4580, Val time: 1.9208\n",
      "====> Epoch: 1270 Total loss: 300307.6591 MSE Loss: 0.0019 MAE Loss: 0.0149 Val loss: 314522.6181\n",
      "====> Epoch: 1280, Train time: 38192.7099, Val time: 1.9186\n",
      "====> Epoch: 1280 Total loss: 299591.7976 MSE Loss: 0.0019 MAE Loss: 0.0149 Val loss: 382638.7300\n",
      "====> Epoch: 1290, Train time: 38493.8508, Val time: 1.9679\n",
      "====> Epoch: 1290 Total loss: 299256.1422 MSE Loss: 0.0019 MAE Loss: 0.0148 Val loss: 318859.6045\n",
      "====Training finish====\n"
     ]
    }
   ],
   "source": [
    "if run_training:\n",
    "\n",
    "    enc, dec, trainloss,valloss = train_model(\n",
    "        model_enc = enc,\n",
    "        model_dec = dec,\n",
    "        epochs = epochs, \n",
    "        training_dataloader = train_loader_sic, \n",
    "        val_dataloader = val_loader_sic,\n",
    "        autograd = optim.AdamW, \n",
    "        #enc_loc = 'outputs/latent_space_verification/',\n",
    "        #dec_loc = 'outputs/latent_space_verification/' , \n",
    "        enc_loc = 'outputs/final/',\n",
    "        dec_loc = 'outputs/final/' , \n",
    "        # trainloss_loc = 'outputs/latent_space_verification/train_loss_', \n",
    "        # valloss_loc = 'outputs/latent_space_verification/val_loss' , \n",
    "        trainloss_loc = 'outputs/final/img_avg_train_loss_', \n",
    "        valloss_loc = 'outputs/final/img_avg_val_loss_' , \n",
    "        mse_multp = 0.0, mae_multp = 1.0, iiee_multp = 1.0,\n",
    "        clamp_img= clamp_yn,\n",
    "        norm_img= norm_yn,\n",
    "        return_model = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAByO0lEQVR4nO3dd3wTdR8H8M8lbboXlC4otEBlyLRsUFD7PEURxYmILFHUBxREZKiAGxVUxIU4wIUgDlRkWAoio7KnbCgUaNMCHelOmtzzx9E0aZM0adOsft6vV19N7n53980Vet/+piCKoggiIiIiNydzdgBERERE9sCkhoiIiDwCkxoiIiLyCExqiIiIyCMwqSEiIiKPwKSGiIiIPAKTGiIiIvIITGqIiIjII3g5OwBH0el0yMzMRFBQEARBcHY4REREZAVRFFFYWIiYmBjIZJbrYhpNUpOZmYnY2Fhnh0FERER1cOHCBbRo0cJimUaT1AQFBQGQbkpwcLCToyEiIiJrqFQqxMbG6p/jljSapKayySk4OJhJDRERkZuxpusIOwoTERGRR2BSQ0RERB6BSQ0RERF5hEbTp4aIiGwjiiIqKiqg1WqdHQp5MLlcDi8vL7tMt8KkhoiIalCr1cjKykJJSYmzQ6FGwN/fH9HR0VAoFPU6D5MaIiIyotPpkJ6eDrlcjpiYGCgUCk5aSg1CFEWo1WpcvnwZ6enpSEhIqHWCPUuY1BARkRG1Wg2dTofY2Fj4+/s7OxzycH5+fvD29sb58+ehVqvh6+tb53OxozAREZlUn7+YiWxhr39r/BdLREREHoFJDRERkRlxcXFYuHCh1eX/+usvCIKA/Pz8BouJzGNSQ0REbk8QBItfL730Up3Ou3v3bkyYMMHq8v369UNWVhZCQkLqdD1rVSZPpr6USmWDXtuVsaMwERG5vaysLP3rlStXYs6cOThx4oR+W2BgoP61KIrQarXw8qr9EdisWTOb4lAoFIiKirLpmPo4ceJEjfUMIyIiTJZVq9Umh0xrNBp4e3vbfO26HteQWFNTTxqtDguWfImtxzONtl/ILcHHf52GqkzjpMiIiBqPqKgo/VdISAgEQdC/P378OIKCgrBu3TokJibCx8cH27Ztw5kzZ3DXXXchMjISgYGB6NmzJzZu3Gh03urNT4Ig4PPPP8fdd98Nf39/JCQk4LffftPvr978tGzZMoSGhmLDhg3o0KEDAgMDMXjwYKMkrKKiAk8//TRCQ0PRtGlTzJgxA2PGjMGwYcNq/dwRERFGnz0qKkrf6Xbs2LEYNmwYXn/9dcTExKBdu3Y4d+4cBEHAypUrMXDgQPj6+uK7776DTqfDK6+8ghYtWsDHxwfdunXD+vXr9dcxd5yrYVJTTykb12HypWmIWX4LPvjwHZzMLgQAPPT5P3h7/Qm88ccxJ0dIRFR/oiiiRF3h8C9RFO32GWbOnIk333wTx44dQ5cuXVBUVITbb78dqamp2L9/PwYPHoyhQ4ciIyPD4nlefvllPPDAAzh06BBuv/12jBw5Erm5uWbLl5SUYMGCBfjmm2/w999/IyMjA9OmTdPvf+utt/Ddd99h6dKl2L59O1QqFVavXm2Xz5yamooTJ04gJSUFa9as0W+fOXMmJk+ejGPHjiE5ORnvv/8+3nnnHSxYsACHDh1CcnIy7rzzTpw6dcrofNWPczVsfqqnm6JFaL380EabhaeuvIItH63Gtq7TcSFXyhc3HstxcoRERPVXqtGi45wNDr/u0VeS4a+wz6PqlVdewX/+8x/9+yZNmqBr167696+++ip++eUX/Pbbb5g0aZLZ84wdOxYjRowAALzxxhtYtGgRdu3ahcGDB5ssr9FosHjxYrRp0wYAMGnSJLzyyiv6/R988AFmzZqFu+++GwDw4YcfYu3atVZ9phYtWhi9b9WqFf7991/9+4CAAHz++ef6Zqdz584BAKZMmYJ77rlHX27BggWYMWMGHnzwQQBSorV582YsXLgQH330kb5c9eNcDZOaegrscgfQ+gDyNy1E0L5PMFB2CDceGoUL8ofxlTYZnISTiMg19OjRw+h9UVERXnrpJfzxxx/IyspCRUUFSktLa62p6dKli/51QEAAgoODkZNj/g9Yf39/fUIDANHR0fryBQUFyM7ORq9evfT75XI5EhMTodPpav1MW7duRVBQkP599T4unTt3NtmPxvBeqFQqZGZmon///kZl+vfvj4MHD5o9zhUxqbGHwGYIvfN1oNcIFH03CoGFZzHX+xvM9f4G80sewLHMnugQ07A94YmIGpKftxxHX3F8c4Oft9xu5woICDB6P23aNKSkpGDBggVo27Yt/Pz8cN9990GtVls8T/XEQRAEiwmIqfL2alaLj49HaGio2f3VP3Nt22tT1+MchX1q7CmqEwKn7kPRLW/oNz3n/QMyFt+Hn9JOWDiQiMi1CYIAf4WXw78acs2p7du3Y+zYsbj77rvRuXNnREVF6ZtnHCUkJASRkZHYvXu3fptWq8W+ffscFkNwcDBiYmKwfft2o+3bt29Hx44dHRaHPbCmxt4EAYE3TQS63YPSLQuh2LsEybJdOLbuXrx37i08Pfx2yGVskyIicraEhAT8/PPPGDp0KARBwOzZs61q8rG3p556CvPmzUPbtm3Rvn17fPDBB8jLy7MqocvJyUFZWZnRtqZNm9o81Pq5557D3Llz0aZNG3Tr1g1Lly7FgQMHXHKEkyVMahpKcDT8hr4FXZe7UfLtCHTQXEDs8Uew8OsPMHnUffCSs5KMiMiZ3n33XTzyyCPo168fwsPDMWPGDKhUKofHMWPGDCiVSowePRpyuRwTJkxAcnIy5PLam97atWtXY1taWhr69OljUwxPP/00CgoK8OyzzyInJwcdO3bEb7/9hoSEBJvO42yCaM/xci5MpVIhJCQEBQUFNSYqanCFSlxd9hCaXt2LLLEJvu70JWbcf7NjYyAislJZWRnS09MRHx9frxWTqW50Oh06dOiABx54AK+++qqzw3EIS//mbHl+s7rAEYKi0PSxX1AU1AbRQi4GH56KA7u3Iy8r3dmRERGRk50/fx6fffYZTp48icOHD+PJJ59Eeno6HnroIWeH5naY1DiKbwgCx/2IYnkIusrOotsftyNwcSIOpTfeNTqIiAiQyWRYtmwZevbsif79++Pw4cPYuHEjOnTo4OzQ3A771DhSk9ZA8hvA2okAAG9Bi20bVqLLE5OdHBgRETlLbGxsjZFHVDesqXGwgO73Gb2PKj3tpEiIiIg8C5MaR/P2RXmbqqm0gwtO2nVtEyIiosaKSY0T+Dy4DNp2dwAAWukuQKkqq+UIIiIiqg2TGmfw9oN8yHwAQLyQhdOZV50cEBERkftjUuMsQdEokQXCS9Dh8rl/ay9PREREFjGpcRZBQF6AtGqrOvOIk4MhIiJyf0xqnEjdVJreWpHLxS6JiFzBoEGDMGXKFP37uLg4LFy40OIxgiBg9erV9b62vc7TmDGpcSK/aGn105DidI6AIiKqh6FDh2Lw4MEm923duhWCIODQoUM2n3f37t2YMGFCfcMz8tJLL6Fbt241tmdlZeG2226z67WqW7ZsGQRBqPHlKcthcPI9J2oS1wlIA1rpLuJKkRrNgnycHRIRkVsaP3487r33Xly8eBEtWrQw2rd06VL06NEDXbp0sfm8zZo1s1eItYqKinLIdYKDg3HihHELgaUVwdVqNRQKhdE2URSh1Wrh5WVbGlHX46zFmhonUkRJNTVxghKnszgCioioru644w40a9YMy5YtM9peVFSEVatWYfz48bh69SpGjBiB5s2bw9/fH507d8b3339v8bzVm59OnTqFm266Cb6+vujYsSNSUlJqHDNjxgxcd9118Pf3R+vWrTF79mxoNBoAUk3Jyy+/jIMHD+prSSpjrt78dPjwYdxyyy3w8/ND06ZNMWHCBBQVFen3jx07FsOGDcOCBQsQHR2Npk2bYuLEifprmSMIAqKiooy+IiMj9fsHDRqESZMmYcqUKQgPD0dycjL++usvCIKAdevWITExET4+Pti2bRvKy8vx9NNPIyIiAr6+vhgwYAB2796tP5e54xoKa2qcKTgGpYI//FCCnPPHgOtinB0REZFpoghoShx/XW9/wEItQiUvLy+MHj0ay5YtwwsvvKCveVi1ahW0Wi1GjBiBoqIiJCYmYsaMGQgODsYff/yBUaNGoU2bNujVq1et19DpdLjnnnsQGRmJnTt3oqCgwKj/TaWgoCAsW7YMMTExOHz4MB577DEEBQVh+vTpGD58OI4cOYL169dj48aNAICQkJAa5yguLkZycjL69u2L3bt3IycnB48++igmTZpklLht3rwZ0dHR2Lx5M06fPo3hw4ejW7dueOyxx2r9PJZ89dVXePLJJ/XLN2RlZQEAZs6ciQULFqB169YICwvD9OnT8dNPP+Grr75Cq1at8PbbbyM5ORmnT59GkyZN9OerflxDYVLjTIKAPP84+BUfRWnmUQC3OjsiIiLTNCXAG074w+v5TEARYFXRRx55BPPnz8eWLVswaNAgAFLT07333ouQkBCEhIRg2rRp+vJPPfUUNmzYgB9++MGqpGbjxo04fvw4NmzYgJgY6V688cYbNfrBvPjii/rXcXFxmDZtGlasWIHp06fDz88PgYGB8PLystjctHz5cpSVleHrr79GQID0+T/88EMMHToUb731lr5mJSwsDB9++CHkcjnat2+PIUOGIDU11WJSU1BQgMDAQKNtN954I9atW6d/n5CQgLffflv/vjKpeeWVV/Cf//wHgJR4ffLJJ1i2bJn+Hnz22WdISUnBF198geeee05/vOFxDYlJjZOVh7YFio/CJ49rQBER1Uf79u3Rr18/fPnllxg0aBBOnz6NrVu34pVXXgEAaLVavPHGG/jhhx9w6dIlqNVqlJeXw9/f36rzHzt2DLGxsfqEBgD69u1bo9zKlSuxaNEinDlzBkVFRaioqEBwcLBNn+XYsWPo2rWrPqEBgP79+0On0+HEiRP6pOb666+HXC7Xl4mOjsbhw4ctnjsoKAj79u0z2ubn52f0PjEx0eSxPXr00L8+c+YMNBoN+vfvr9/m7e2NXr164dixY2aPa0hMapxMbNYOuASEFp91dihEROZ5+0u1Js64rg3Gjx+Pp556Ch999BGWLl2KNm3aYODAgQCA+fPn4/3338fChQvRuXNnBAQEYMqUKVCr1XYLNy0tDSNHjsTLL7+M5ORkhISEYMWKFXjnnXfsdg1D3t7eRu8FQYBOp7N4jEwmQ9u2bS2WMUymrNlem7oeZyt2FHYy3+gOAIAodYaTIyEiskAQpGYgR39Z0Z/G0AMPPACZTIbly5fj66+/xiOPPKLvX7N9+3bcddddePjhh9G1a1e0bt0aJ0+etPrcHTp0wIULF/RNMQDwzz//GJXZsWMHWrVqhRdeeAE9evRAQkICzp8/b1RGoVBAq9XWeq2DBw+iuLhYv2379u2QyWRo166d1TE3pDZt2kChUOj73QCARqPB7t270bFjR6fEVKek5qOPPkJcXBx8fX3Ru3dv7Nq1y2L5VatWoX379vD19UXnzp2xdu1ao/0///wz/vvf/6Jp06YQBAEHDhyocY6ysjJMnDgRTZs2RWBgIO69915kZ2fXJXyXEhx7PQAgDpkoLa9wcjRERO4tMDAQw4cPx6xZs5CVlYWxY8fq9yUkJCAlJQU7duzAsWPH8Pjjj9v0HElKSsJ1112HMWPG4ODBg9i6dSteeOEFozIJCQnIyMjAihUrcObMGSxatAi//PKLUZm4uDikp6fjwIEDuHLlCsrLy2tca+TIkfD19cWYMWNw5MgRbN68GU899RRGjRplNFKpLkRRhFKprPFVWw1PdQEBAXjyySfx3HPPYf369Th69Cgee+wxlJSUYPz48fWKsa5sTmpWrlyJqVOnYu7cudi3bx+6du2K5ORk5OTkmCy/Y8cOjBgxAuPHj8f+/fsxbNgwDBs2DEeOVC0NUFxcjAEDBuCtt94ye91nnnkGv//+O1atWoUtW7YgMzMT99xzj63hu5yAJtEAAD9Bjav5+c4NhojIA4wfPx55eXlITk426v/y4osv4oYbbkBycjIGDRqEqKgoDBs2zOrzymQy/PLLLygtLUWvXr3w6KOP4vXXXzcqc+edd+KZZ57BpEmT0K1bN+zYsQOzZ882KnPvvfdi8ODBuPnmm9GsWTOTw8r9/f2xYcMG5ObmomfPnrjvvvtw66234sMPP7TtZpigUqkQHR1d48vcc9ySN998E/feey9GjRqFG264AadPn8aGDRsadISTJYJo41S2vXv3Rs+ePfU3VqfTITY2Fk899RRmzpxZo/zw4cNRXFyMNWvW6Lf16dMH3bp1w+LFi43Knjt3DvHx8di/f7/RbIsFBQVo1qwZli9fjvvuuw8AcPz4cXTo0AFpaWno06dPrXGrVCqEhISgoKDA5g5bDUoUUfFyE3hBhyMP7kKn9q5RrUhEjVdZWRnS09MRHx/vMTPNkmuz9G/Olue3TTU1arUae/fuRVJSUtUJZDIkJSUhLS3N5DFpaWlG5QEgOTnZbHlT9u7dC41GY3Se9u3bo2XLlmbPU15eDpVKZfTlkgQBJYI0tC4/1/YsmYiIiCQ2JTVXrlyBVqut0Z4XGRkJpVJp8hilUmlTeXPnUCgUCA0Ntfo88+bN089LEBISgtjYWKuv52jlXkEAgAMnzzk3ECIiIjfmsaOfZs2ahYKCAv3XhQsXnB2SWYoAaTbJzlk/OjkSIiIi92XTPDXh4eGQy+U1eotnZ2ebnRkxKirKpvLmzqFWq5Gfn29UW2PpPD4+PvDxcY8FIn0rCgAAMepzzg2EiIjIjdlUU6NQKJCYmIjU1FT9Np1Oh9TUVJOzKgLSbIuG5QEgJSXFbHlTEhMT4e3tbXSeEydOICMjw6bzuKqivtMBAFqdDjb22yYiIqJrbJ5ReOrUqRgzZgx69OiBXr16YeHChSguLsa4ceMAAKNHj0bz5s0xb948AMDkyZMxcOBAvPPOOxgyZAhWrFiBPXv2YMmSJfpz5ubmIiMjA5mZ0myVlUuiV64eGhISgvHjx2Pq1Klo0qQJgoOD8dRTT6Fv375WjXxydQHNpUmKAlGCovIKBPl613IEEVHD4x9Z5Cj2+rdmc1IzfPhwXL58GXPmzIFSqUS3bt2wfv16fWfgjIwMyGRVFUD9+vXD8uXL8eKLL+L5559HQkICVq9ejU6dOunL/Pbbb/qkCAAefPBBAMDcuXPx0ksvAQDee+89yGQy3HvvvSgvL0dycjI+/vjjOn1oV+MbKK1kGowSKAvKmNQQkVNVTr1fUlJSY00gooZQUiKtAF992Qdb2TxPjbty2XlqAKDoMrCgLXSigC0jjuPm9tb3NyIiaghZWVnIz89HREQE/P399UsNENmTKIooKSlBTk4OQkNDER0dXaOMLc9vLmjpCnylH5JMEFGccQhgUkNETlY5CKMus8wS2So0NNSmAUTmMKlxBV4+KJEHw1+rQtNzvwP4r7MjIqJGThAEREdHIyIiAhqNxtnhkAfz9vaGXC63y7mY1LiIf1uNRs+zHyKowPoVY4mIGppcLrfbA4eooXns5HvupiJWGsUVWXrGyZEQERG5JyY1LiIotgsAoJnuMlCa79xgiIiI3BCTGhcRHRWFTFEa2q1WHnVyNERERO6HSY2LaBKgQA6aAgDyczKdHA0REZH7YVLjIgRBQIlXKABAlZvl3GCIiIjcEJMaF6LxlWpqSvOyaylJRERE1TGpcSFiQDgAoKKQk10RERHZikmNC5EHSbMpyoqUTo6EiIjI/TCpcSGK8DgAQGAZOwoTERHZikmNCwmKag0AaKphTQ0REZGtmNS4kGax1wEAQlGIq7lXnRwNERGRe2FS40KahTeDSggCAOw7eNDJ0RAREbkXJjUuRuUTDQAozTnr5EiIiIjcC5MaF1MeEAMAuCF9iZMjISIici9MalyMENAMANCi7ASQy9oaIiIiazGpcTGR/lWvCy9fcF4gREREboZJjYsJCG6if30567wTIyEiInIvTGpczY1T9S8Lc1hTQ0REZC0mNa4mKAo7wu8DAJSqLjs5GCIiIvfBpMYF+QaGAgDKi/KdGgcREZE7YVLjggKCwwAAFaUFTo6EiIjIfTCpcUER4eEAgFvLN+H8lSInR0NEROQemNS4oLCwcP3rK2nfOjESIiIi98GkxhV5+ehfhp75zYmBEBERuQ8mNa5IV6F/mV9QAFEUnRgMERGRe2BS44quuw2iXKqtaabNRn6JxskBERERuT4mNa7I2xfC0/sBADHCVSjz2VmYiIioNkxqXFVQFDTwgpegw5I/tjs7GiIiIpfHpMZVyeTIkUmjoEqzjjs5GCIiItfHpMaFBUa3AwAsFl/Dxc2fA8VXnRwRERGR62JS48ICB0zQv26x5VlgfmuAI6GIiIhMYlLjwuTXDa65UVPq+ECIiIjcAJMaVyb3gnrYZ8bbNCXOiYWIiMjFMalxcYr2yUbvS4q4yCUREZEpTGpcnW+I0dsL2VecFAgREZFrY1LjZrIuM6khIiIyhUmNmykqZPMTERGRKUxq3MDZdo/pX5cXFzoxEiIiItfFpMYNtL7/Df3r8lImNURERKYwqXEHXgqcj5bmrPEuVjo5GCIiItfEpMZNVDRJAACEFKc7ORIiIiLXxKTGTQQ07wgAiCg/D52OSyUQERFVx6TGTYTHdwEAtMZFZOZzVmEiIqLqmNS4Ca9mCdBChmChFBnnTzs7HCIiIpfDpMZdePlAqWgFACg8u9fJwRAREbkeJjVupCCsEwBAzNzv5EiIiIhcD5MaN6KIvQEAEJR7GBqtzsnREBERuRYmNW4ktvONAIBuuqPYd/ysk6MhIiJyLUxq3IhPyx5QejVHgFCO8rPbnR0OERGRS2FS404EAVcDpEn4NFfPOTcWIiIiF8Okxs1UBMcCAGQFF5wcCRERkWthUuNm5E3iAAD+JRedGwgREZGLYVLjZvwj4gEAoWoubElERGSISY2badpC6lMTpcvGlaJyJ0dDRETkOpjUuJmQ6DbSd6EE2w5zuQQiIqJKTGrcjSIARV6hAIBmRz53bixEREQuhEmNGyrzjwEA9L/0JZCb7uRoiIiIXAOTGjfkrfCpelNy1XmBEBERuRAmNW5IvO52/WttWZETIyEiInIdTGrcUNAtz+hf5+VedmIkREREroNJjRuSe3ljl1ciAODq1StOjoaIiMg11Cmp+eijjxAXFwdfX1/07t0bu3btslh+1apVaN++PXx9fdG5c2esXbvWaL8oipgzZw6io6Ph5+eHpKQknDp1yqjMyZMncddddyE8PBzBwcEYMGAANm/eXJfwPYNPMACgIJ99aoiIiIA6JDUrV67E1KlTMXfuXOzbtw9du3ZFcnIycnJyTJbfsWMHRowYgfHjx2P//v0YNmwYhg0bhiNHjujLvP3221i0aBEWL16MnTt3IiAgAMnJySgrK9OXueOOO1BRUYFNmzZh79696Nq1K+644w4olY1zZl25fwgAoLwoz8mREBERuQjRRr169RInTpyof6/VasWYmBhx3rx5Jss/8MAD4pAhQ4y29e7dW3z88cdFURRFnU4nRkVFifPnz9fvz8/PF318fMTvv/9eFEVRvHz5sghA/Pvvv/VlVCqVCEBMSUmxKu6CggIRgFhQUGDdB3Vxez9/ShTnBotbFj3q7FCIiIgajC3Pb5tqatRqNfbu3YukpCT9NplMhqSkJKSlpZk8Ji0tzag8ACQnJ+vLp6enQ6lUGpUJCQlB79699WWaNm2Kdu3a4euvv0ZxcTEqKirw6aefIiIiAomJibZ8BI8h85Oan2RqlZMjISIicg1ethS+cuUKtFotIiMjjbZHRkbi+PHjJo9RKpUmy1c2G1V+t1RGEARs3LgRw4YNQ1BQEGQyGSIiIrB+/XqEhYWZvG55eTnKy6vWRlKpPOvh7+UnfW4vdaGTIyEiInINbjH6SRRFTJw4EREREdi6dSt27dqFYcOGYejQocjKyjJ5zLx58xASEqL/io2NdXDUDUsRGCp9r+A8NURERICNSU14eDjkcjmys7ONtmdnZyMqKsrkMVFRURbLV363VGbTpk1Ys2YNVqxYgf79++OGG27Axx9/DD8/P3z11Vcmrztr1iwUFBTovy5cuGDLR3V5/sFNAAA3aA9CLGFnYSIiIpuSGoVCgcTERKSmpuq36XQ6pKamom/fviaP6du3r1F5AEhJSdGXj4+PR1RUlFEZlUqFnTt36suUlJRIwcqMw5XJZNDpdCav6+Pjg+DgYKMvT9IsvJn+tWbFaCdGQkRE5Bps6lMDAFOnTsWYMWPQo0cP9OrVCwsXLkRxcTHGjRsHABg9ejSaN2+OefPmAQAmT56MgQMH4p133sGQIUOwYsUK7NmzB0uWLAEg9ZeZMmUKXnvtNSQkJCA+Ph6zZ89GTEwMhg0bBkBKjMLCwjBmzBjMmTMHfn5++Oyzz5Ceno4hQ4bY6Va4F5/gqqRGkfG3EyMhIiJyDTYnNcOHD8fly5cxZ84cKJVKdOvWDevXr9d39M3IyDCqUenXrx+WL1+OF198Ec8//zwSEhKwevVqdOrUSV9m+vTpKC4uxoQJE5Cfn48BAwZg/fr18PX1BSA1e61fvx4vvPACbrnlFmg0Glx//fX49ddf0bVr1/reA/cU2srZERAREbkUQRRF0dlBOIJKpUJISAgKCgo8pynqpRCD1wXOi4OIiKiB2PL8dovRT2Ta6aY3AwCuKJo7ORIiIiLnY1Ljxi62k/oxaUz3lSYiImpUmNS4sSZhoQAAL22Z5YJERESNAJMaN9asiTSrsEJXBp2uUXSNIiIiMotJjRsLv5bU+KEMV4rLaylNRETk2ZjUuDFv3yAAgELQIutKvnODISIicjImNe7MNwRFQiAAoOCi6QVFiYiIGgsmNe5MEJDjEwcA0CiPOjcWIiIiJ2NS4+bK/aVFP9UF2bWUJCIi8mxMatycl7/Ur6a0WOXkSIiIiJyLSY2b8/WXpoyuKGVSQ0REjRuTGjfnHyit/6QtK3JyJERERM7FpMbNBQaHAgC8tSUoKq9wbjBEREROxKTGzflca37yRxmUBVwugYiIGi8mNe5OIc1TE4hS1tQQEVGjxqTG3flIo59ChGIUM6khIqJGjEmNu2vWDgBwnXARRSVsfiIiosaLSY27a9IGZYIv/AQ1xLzzzo6GiIjIaZjUuDuZDKVyqQlKU1rg5GCIiIich0mNB1DL/QEAFaWFTo6EiIjIeZjUeICKyqSmjEkNERE1XkxqPIDWS0pqRM4qTEREjRiTGg8gKgIAALpyJjVERNR4ManxAKK3lNRAU+zcQIiIiJyISY0nuDarsEzNpIaIiBovJjUeQPCT1n9SaDikm4iIGi8mNR5ADGoOAAjVZDs5EiIiIudhUuMBfMLjAABhmmxcyC1xbjBEREROwqTGA0S3ug4AEC8ocTyLTVBERNQ4ManxBM3ao0zwQbBQAvnVE86OhoiIyCmY1HgCuTcuKVoDALzzzjg5GCIiIudgUuMhChURAAB5sdLJkRARETkHkxoPUeLTDADgXZLj5EiIiIicg0mNhyjziwQABBenOzkSIiIi52BS4yGym/QEALTJ3w5oNU6OhoiIyPGY1HiI0tD2AAAvsQLQcK4aIiJqfJjUeIigwICqNxVq5wVCRETkJExqPERkiB/KRS/pjbbcucEQERE5AZMaDxEV4gs1vKU3FUxqiIio8WFS4yEig3xRfi2pKS9jnxoiImp8mNR4iGA/L2ggNT/lqoqcHA0REZHjManxEIIgQCtTAADyVIVOjoaIiMjxmNR4EJ3cBwCQX8iaGiIianyY1HgQP0GadC/65HdOjoSIiMjxmNR4kGaaTABAfM5GJ0dCRETkeExqiIiIyCMwqSEiIiKPwKSGiIiIPAKTGg/yT+K7VW90OucFQkRE5ARMajxIadytVW80xc4LhIiIyAmY1HiQTq2ioBUFAMDVvDwnR0NERORYTGo8SLNgX5QK/gCAA6cynBwNERGRYzGp8TAaRQgA4GLmJSdHQkRE5FhMajxMuSIMACCUXHVyJERERI7FpMbDqH2aAAC8y5jUEBFR48KkxsNU+EpJjaI818mREBERORaTGg+jC2gGAAhQX3FyJERERI7FpMbDVIS0AgA0vba4JRERUWPBpMbThMUDADpq/gW0GicHQ0RE5DhMajyMENMValGOAJRCPPWns8MhIiJyGCY1HiauRXOkiL0AAJfTDzk5GiIiIsdhUuNhfLzkKAxqCwAozzrh5GiIiIgch0mNB9IExgAAhCKlkyMhIiJyHCY1HkgeHAkA8OIEfERE1IgwqfFAfiFSUuNbzqSGiIgajzolNR999BHi4uLg6+uL3r17Y9euXRbLr1q1Cu3bt4evry86d+6MtWvXGu0XRRFz5sxBdHQ0/Pz8kJSUhFOnTtU4zx9//IHevXvDz88PYWFhGDZsWF3C93ht27QGAARU5KOiosLJ0RARETmGzUnNypUrMXXqVMydOxf79u1D165dkZycjJycHJPld+zYgREjRmD8+PHYv38/hg0bhmHDhuHIkSP6Mm+//TYWLVqExYsXY+fOnQgICEBycjLKysr0ZX766SeMGjUK48aNw8GDB7F9+3Y89NBDdfjInu/6a0mNt6DFlaumfy5ERESeRhBFUbTlgN69e6Nnz5748MMPAQA6nQ6xsbF46qmnMHPmzBrlhw8fjuLiYqxZs0a/rU+fPujWrRsWL14MURQRExODZ599FtOmTQMAFBQUIDIyEsuWLcODDz6IiooKxMXF4eWXX8b48ePr9EFVKhVCQkJQUFCA4ODgOp3DnRS+FI0glOD4valo37mHs8MhIiKqE1ue3zbV1KjVauzduxdJSUlVJ5DJkJSUhLS0NJPHpKWlGZUHgOTkZH359PR0KJVKozIhISHo3bu3vsy+fftw6dIlyGQydO/eHdHR0bjtttuManuqKy8vh0qlMvpqTFSyUABA0dUs5wZCRETkIDYlNVeuXIFWq0VkZKTR9sjISCiVpocPK5VKi+Urv1sqc/bsWQDASy+9hBdffBFr1qxBWFgYBg0ahNxc06tRz5s3DyEhIfqv2NhYWz6q2ytRSKt1F+cxqSEiosbBLUY/6XQ6AMALL7yAe++9F4mJiVi6dCkEQcCqVatMHjNr1iwUFBTovy5cuODIkJ1O9A8HAOTlXHJyJERERI5hU1ITHh4OuVyO7Oxso+3Z2dmIiooyeUxUVJTF8pXfLZWJjo4GAHTs2FG/38fHB61bt0ZGRobJ6/r4+CA4ONjoqzHxCZMm4CtlTQ0RETUSNiU1CoUCiYmJSE1N1W/T6XRITU1F3759TR7Tt29fo/IAkJKSoi8fHx+PqKgoozIqlQo7d+7Ul0lMTISPjw9OnKia9l+j0eDcuXNo1aqVLR+h0QhsIiU13qWXYWNfcCIiIrfkZesBU6dOxZgxY9CjRw/06tULCxcuRHFxMcaNGwcAGD16NJo3b4558+YBACZPnoyBAwfinXfewZAhQ7BixQrs2bMHS5YsAQAIgoApU6bgtddeQ0JCAuLj4zF79mzExMTo56EJDg7GE088gblz5yI2NhatWrXC/PnzAQD333+/Pe6Dxwlu1hwAEKrLQ26xGk0DfZwcERERUcOyOakZPnw4Ll++jDlz5kCpVKJbt25Yv369vqNvRkYGZLKqCqB+/fph+fLlePHFF/H8888jISEBq1evRqdOnfRlpk+fjuLiYkyYMAH5+fkYMGAA1q9fD19fX32Z+fPnw8vLC6NGjUJpaSl69+6NTZs2ISwsrD6f32N5h0hNdpFCHs7nljCpISIij2fzPDXuqrHNU4Pso8AnfVEg+mPTsN24u3sLZ0dERERkswabp4bcSJjU1yhEKIFSyc7CRETk+ZjUeCpFAFS+Umfh4AubnBwMERFRw2NS48FyWt4BAIjK3e3kSIiIiBoekxoP5tOiCwCgSVkGxJJcYOenQPFVJ0dFRETUMJjUeLCIeGmEWUsxE+UrHwHWTQd+esTJURERETUMm4d0k/vwibwOANBUKATOb5Y2nv3LeQERERE1INbUeDJFAPK8wp0dBRERkUMwqfFw5VDU3Ng4piYiIqJGhkmNhwvyMzGT8PE/HB8IERFRA2NS4+ECOg2puZH9aoiIyAMxqfF0g2bV3ObtW3MbERGRm2NS4+l8ArHPp6fxNi8/58RCRETUgJjUNAIV3kHGG1hTQ0REHohJTSPQNDjAeANraoiIyAMxqWkE4iNCjDeIOucEQkRE1ICY1DQCMrnxxNGiVu2kSIiIiBoOk5rGQGac1BQVF9d+TMZOYM+XnKiPiIjcBpOaxqD/ZEBR1Vn4ckFR7cd8+V9gzTPA2c0NGBgREZH9MKlpDEJbAtPPIiVsOACgqKTE+mOvnmmgoIiIiOyLSU1j4aWAv5806qmkpNTJwRAREdkfk5pGJMDfHwBQVsakhoiIPA+TmkYkOEBKasrLypwcCRERkf0xqWlEggOlpKakrBRaHUc1ERGRZ2FS04iEBAYCAAbIjuDHPRlOjoaIiMi+mNQ0It4xnQEAzYQCFB/6zcnREBER2ReTmsakVT+UBrQAADxy8UWg4JKTAyIiIrIfJjWNjK7v0/rX5X++zBmDiYjIYzCpaWQCBjyO7V59AAA+/64EDv3g5IhsVF7o7AiIiMhFMalphEKaX6d/XXFiQ80Crlp7s+k1YF4L4PhaZ0dCREQuiElNIxQc0Ur/+tJVVc0kRqd1cERW+nu+9H3ddOfGQURELolJTSMU0zxW/zon6zx0iwcAP46vKiC6aFKjJzg7ACIickFMahohL4Wf/nVP2UnIso8AR36sKiDqnBAVERFR/TCpaYxiupveXqGWvrtq81MlgTU1RERUE5Oaxig0FprH/kam2MR4u7pI+u7qzU9MaoiIyAQmNY2Ud/Ou0LYaaLzx8Crpu6vX1BAREZnApKYRa962s/GGddOB0nw36FPDmhoiIqqJSU0jJmvapubGkqvGNTWuOmcNERFRNUxqGrPWg2puK8037lPjiv1r2KeGiIhMYFLTmPmFAk9sM96Wfx449nvVe5fsX8OkhoiIamJS09hFdcZXNxis//TjOOMZe1lTQ0REboJJDaFZfBfzO12ypoaIiKgmJjWEwddH4f2Ku03vdMmRUKypISKimpjUEGQyAYMnvm965/ZF5g/UaYGygoYJyhI2PxERkQlMaggA0C46xPSOcgtJy1d3Am+2BPLON0xQho78ZPDGTknNmU3AqrFA8VX7nI+IiJyKSQ3pfdPxM9M7tBWmt5+/NnLq8A+m99vTj4/Y/5zf3A38+wuw4Xn7n5uIiByOSQ3p3XXn3Tila15zh6bY8oE6B/e7qWx+stfEgKpL9jkPERE5FZMa0gv29Uaxb2TNHeoSywc6fNi3ABxYDrwdD1zYVf/TcdZkIiKPwKSGjDQPUdTcqKklqXH0sG9BAFY/CZTmAT+Mcey1iYjIZTGpISNNmifU3LhzMXBpn/mDnFFTU3VxB1+biIhcFZMaMiL/z8s1N+5aIo0SMocT9BERkQtgUkPGApoCM87V3J5/bdj27i+AlQ8DFeqqfY6eoM/u89SwtoeIyBN4OTsAckF+Ydje/gX0P/668faFnYH8DOn1we+rtrOmhoiIXABrasikHvc9i72yzsYbKxMaACjNrXrt8KUUOKNwgyovBDa9BiiPODsSIiKbMKkhk3y85FDfuRh/azubLmA4IZ+jOwozp2lYqa8Cf88HFvd3diRERDZhUkNm9e3WCc97TTO98+qpqtcOb36yc1bDeWqMZR1wdgRERHXCpIYs+mz8jaZ3HFpZ9bo+NTWa0rofS0REZIBJDVnUoUXT2gvt+xrYttD2k29+A3g9Cji3zbbj7D36iat+ExF5BCY1ZB8b50rfdVpgzVTg4Iraj9nylvR93UwbL8YkhIiIamJSQ/Z19FdgzxfAL4833DXsXbPCPjVERB6BSQ1ZbY1Y+2iYkisX6nBmW5MK1tQQEVFNnHyPavdoKpB9BNvSu0M4OA1D5GZWxi7KQcn+H+Bf+V4UratVqU9NCWtZiIjoGiY1VLsWPYAWPfB6dxFt9kxBW2E62sku1iz3xX8QXnCu6n1FGeDtZ8UFbExM2LGXiIhMYPMTWU0uE/BQ75Z4XPOM6QJ554zflxeZLqetAH4YU49IuPYTERHVxKSGbPLaXZ0wsE8fXBWDai0rlhcCZaqaO06uA46uNihoJqlQHgb++cR49mKANTVERGQSm5/IJjKZgPEDWqNwrz+aCoUWy+r+fgfyg98CwxYD3UZU7VAXVytpJqlZPED6Lveue8BERNRo1Kmm5qOPPkJcXBx8fX3Ru3dv7NplpuPoNatWrUL79u3h6+uLzp07Y+3atUb7RVHEnDlzEB0dDT8/PyQlJeHUqVMmz1VeXo5u3bpBEAQcOHCgLuFTPbVs6o+3/aagQPS3WE5+8FvpxeonLJ+wts6+WQerbWBNDRER1WRzUrNy5UpMnToVc+fOxb59+9C1a1ckJycjJyfHZPkdO3ZgxIgRGD9+PPbv349hw4Zh2LBhOHKkagXgt99+G4sWLcLixYuxc+dOBAQEIDk5GWVlZTXON336dMTExNgaNtnZoulP4BH56465mOHq4OQATBqJyD3ZnNS8++67eOyxxzBu3Dh07NgRixcvhr+/P7788kuT5d9//30MHjwYzz33HDp06IBXX30VN9xwAz788EMAUi3NwoUL8eKLL+Kuu+5Cly5d8PXXXyMzMxOrV682Ote6devw559/YsGCBbZ/UrIrL7kMI3q3stPZaqmpOfuX8XtOvkdERCbYlNSo1Wrs3bsXSUlJVSeQyZCUlIS0tDSTx6SlpRmVB4Dk5GR9+fT0dCiVSqMyISEh6N27t9E5s7Oz8dhjj+Gbb76Bv7/lZg9AaqZSqVRGX2Rfd3SrSmp6lH2if71F28XygdWTCJuTCtYkEBFRTTYlNVeuXIFWq0VkZKTR9sjISCiVSpPHKJVKi+Urv1sqI4oixo4diyeeeAI9evSwKtZ58+YhJCRE/xUbG2vVcWQ938i2OBg3Hq9oRuEKQnB3+cuYEvYhVKg96TRWj3lqDF9nHQS2vA1oajZbEhGR53OLId0ffPABCgsLMWvWLKuPmTVrFgoKCvRfFy7UZfp+qk3Xse/i6RfeBQDsFxOwOqsJisRaJtzTltfvohmmawXx6U3A5teBHYvqd353wCYzIqIabEpqwsPDIZfLkZ2dbbQ9OzsbUVFRJo+JioqyWL7yu6UymzZtQlpaGnx8fODl5YW2bdsCAHr06IExY0xP4ubj44Pg4GCjL2oYof4K9IwL078vgW/NQpf2Sd+vngF+n2y8z94PaOVhGw9wswQhYyfwVhyw/ztnR0JE5FJsSmoUCgUSExORmpqq36bT6ZCamoq+ffuaPKZv375G5QEgJSVFXz4+Ph5RUVFGZVQqFXbu3Kkvs2jRIhw8eBAHDhzAgQMH9EPCV65ciddfd9AIHLLou0f7YPXE/oht4gc/mKiJ+exmYNkdwAc3mDjazkmFMybnqz5BYEP6YRRQlg/8+j/HXZOIyA3YPPne1KlTMWbMGPTo0QO9evXCwoULUVxcjHHjxgEARo8ejebNm2PevHkAgMmTJ2PgwIF45513MGTIEKxYsQJ79uzBkiVLAACCIGDKlCl47bXXkJCQgPj4eMyePRsxMTEYNmwYAKBly5ZGMQQGBgIA2rRpgxYtWtT5w5P9KLxk6BYbiq3Tb8HPn+0ALm2qWejcVtMH27umRnBwq2rBReDjfkD3kcDgeQ1/PTY9ERGZZHNSM3z4cFy+fBlz5syBUqlEt27dsH79en1H34yMDMhkVQ+Vfv36Yfny5XjxxRfx/PPPIyEhAatXr0anTp30ZaZPn47i4mJMmDAB+fn5GDBgANavXw9fXxPNGOTyStrfh78y/sQgefVJ88yx9yrdBjU1Oq1UQxQcA9z3hbRNWwHo7FizsvVdoLwA+OdjxyQ1XCaCiMikOi2TMGnSJEyaNMnkvr/++qvGtvvvvx/333+/2fMJgoBXXnkFr7zyilXXj4uLg8i/Vl3WfX0S8Ow/ozGo+FnbDqxQA98MA6K7Aslv1D0Aw5oa5SEgY8e1wK4lNR/3AfLPV5Wp77+lCgePtnJ0TVR9HfoBCE8AYro7OxIi8nBu9tuR3IGvtxz3JP/H+gPyzgGb35BGNZ3fLtV4aErqHoBhTYZOV/W6Mnm5egrQqut+/uocndS40zw957YDPz8GLBnk7EiI7Cf9b+CT/sAFy0sEkeMxqaEGcXOnlvjB/0HrD9jyFrD/26r3V07W4+oGD311UdVrnbYe57TA0fPiuFPz0+Xjzo7A/k5vBPaYnkGdGomvhgLZR6SmbXIpXKWbGoRMJmDQk+8D76yw/qBLe6pe2/IX0JGfgHKD5KXyoV9wCfj6zqrtohYQ5daf11qsqWlcvr1X+t48UWoqpcarvnNukd2xpoYaTESQL672moZjulh0LVuCHb4DLR9Qml/1eu9X1l1E1AE/PgL8/nTVtso+J4dXGZfVVTRMbQ371JjnTrVKtlJlOTsCIqrGjX47kjtqevtsXHwwFQUIxOGiQMuFS3OrXuf8a90FdBoTG689SGXVKiJ1WvuOeqrk8KTGsZcjInIXTGqowf2nYyR+fKIvsmWRtRe2VYWJDr+VNRmyak1NugqpCcreHL7WlDtlNe4UKxG5OyY15BA94prgzvvG2v/EpkYxVT5Hra2pKblSvxisqanRlAEXdhuPxqord2p+IiJyIP52JIfp1qUrLt25Eg9VzEVS+dv2Oamp5idzNTWi1nSfmtyzxv15bGVN7c+qMcAXScD2hXW/TiV36qfiTrESkdtjUkMO1fyGwfju1WcQF9em4S6Smw6c/NNETY2FjsLZR4AylfR67zLgvc7A5foMK6/m5Hrp+z+f2OFkDZwoMBEhIjfFpIYcThAEDOnVHqPUM7Ff19b+Fzi3FVh+P3Bxj/F2S31q1s8C3owFzm2TVhEvyAD+mGrd9Rw9u7VbNT95cILE5I/I5bjTb0fyIMO6NcekRydggnoqVKJfw1wkc5/xe0ujn5SHpO+bXqvaVmHtHBSOTmrc/GFafBVYervxZIvuwiiBdfOfg7V2fwGcWO/sKIiswqSGnEIQBPRu3RQz7x+IG7X2aJIxoXpTk85MnxpzDGtEKtSAKtN0OUfmNBXl9Zxt2cFMJWBb3pSWw/h1ouPjqa/Gtuac8ohUY/n9cGdHQmQVJjXkVPcmtsCnj9yEf2QNsNhh9VoZXQVQXmj98YZJzZfJwLsdgCxTK4/b8qCr50Px8I/1O94VlBU4O4K6E+0wes2dFCmdHQGRTZjUkNP1ad0UHR96E+sUyRhY/q79TqytNjLq2G/A4v61HGRQs2BYy1DZlHXoh5qH1Pevd1UmsHa6dR2T1cX1u5bDGdxDT6jlaGxJDZGbYVJDLiG4bR/0mfwtLiDKfietXlPz1zwbT2Ci6aT6iCqg/g+6VeOAXZ8Cn99ae1l3fqg21IKijmR4/929b5NVGsNnJE/CpIZcRliAAideuw2pofcBAGZqHq3fCavX1NjK1ENL7l2/c5qqrbi4W/pergL+fBEoyrFwvJslNYb30N1iN6UhZqQmIrthUkMuxVsuQ58JH2J8yOdYo+1jvLPtf2w7WV36Axg+hE0NnTZVU1PvnsIGx+/4APh5goWi7vZQNUxq3C12EzwhMasrdYmzIyCqFZMacjkB/n74aNI9aNeqOd7R3Kff/kfRdQ1/8fPbq16nbwG0Fca1K4ZJjSgCa54BCm1YrbmiTJqA7+oZ82Uqa25McaWHatZBKQHLzzBfxjBJvLDLvTsJA9XufyNrmvn6TmdHQFQrJjXkkny95fjxib7I6zkFfco+wJ3lr+LHDH/HB3I6BfhhVNV7w+ans5uBPV/adj51EbB+JvDBDUDJtVXJqzdJWepQ60pJzac3AYdWAqvGWlf+6zulY9yZK91/R7OUbDtLmQr48jZg56fOjoRcBJMaclmCIOC1YZ2x5bWRSP7v7VCKTRwfxNZ3gWO/V72XGSQ1lUmJIcMFK8+nASsfBvIvmD73b0/ZHo8rPlQvn7Cws1ptRt45686prQCunK5rRA3HMOF0xYqaK6elh/zpjc6OxDHSPgQydgDrpjs7EnIRTGrI5fl4yTHx5rZ4dtS9+Kr1fNxZ/qrR/pHqWRBb9QMiOtr/4gUXjd8bNqeYmp3YMOlYOlhKiFY/afrcx9dIq3fbwlRSU1YA/DAaOPqbbeeyl4YYqv3TI8CHicD+7+x/7vowvP+uOEL9p/HSQ/7be50diWOUFzk7AsfaPA/4dGDj+9w2YFJDbiOpYyTGjJ6AO2+/A9+ItwMAxqqnY7uuM3YN/BaI6mz/i2qrLZVgOKLKZFJjojPspb3mz3/kJ9j0dDSVQPy9ADj6q3EzmTXKC+s/QqyhHP1V+m6PVc3tySipdMGsxtLIOXJ/W94Esg4A+79xdiQui0kNuZ1Hb2yNUS9/j09v3oe/dN0AAMOX/IPxF25DeeQNQPNE+12s+gzEOsOkxkQCY6omRWNh1IhWbV0cx/8Aco6bPn9RtnXnqHT4R2DnEmBeC+Cj3rYda5Klh3s9H/yuNmGf4c/c2qbAyyelfkfZ/zZISE5zcKVzm7mO/AT885Hzru9M1v7eaISY1JDbenxgG/z5zE0I8pFGJKVmKdDu/DT82vMboN0Q+1yk+i8PrUHtjKmaGlsnmDM1742m2Hjm4Ix/gBUPAR/3NvMgtaFzh7pYaqJY95z0PtfCKCx7qG8fIFfrQ2TU/GRlbN/eC/z7C/BFcsPE5AxXzwC/THBuM9ePjzjv2uSymNSQW7suMghpz9+K8QPi9dsmrziAfecuN8wF61JTY4nJeW8grYxcyXC9KVPnt2VmW1Mrjx9caf3xpjToaC0Xq6kxSmqsjK3g2pB3tQ3rjtWZg+6XLdMYVDr0A7BkkOUpAIjqiUkNub1AHy/MvqMjdj5/K7xk0gM+s6TqQV8h2vGfuWEflOr9bQDbJ5gzV7Nj1KRkkLTsq9aWbq5PTGG29Q/dXyZUjeRK3wpkpFl3nDVYU+M6sg4CWYfsc666NAv+/BiQuR9YN8M+MRCZwKSGPEZksC+OvToYTw5qgzcrHsIpXXPM0oxHkno+UrQ3mD4oto/p7eaU5gFnNgOFSuDMppr7bf1lv3ep6e1l+dLIq/Stxturz5JcmocazU97vgTeuQ7Yubjmec0lUZWT4n11R20Rm9CANTWu1qfGXZMadbE0R9CnN5qurbNZPX4u1fupNYSyAmDxAODv+Q1/LXIpTGrIo3jLZZgxuD22zRuL7FF/I9X/dpwTo/GY5lm8WFGtDX7SXuCBr227wL6vgG+GAe91Mp3U2NqnJjfd9PayAuC966Ukw9KkZyVXazY/rZ917ftME/GZ6AcEmI/bXFJxcoP5mIyOd4OkRl0i/SwrrOh86eikRlsBpP9t/RIF5u5XaX7Va3us9F6XZjirzisCmQfqP/P0ziWA8jCw6TW7hOUUV88A73cF/jHxx4k9FV8F/pgm3XcPwKSGPNaAhHDseiEJ/8y6Fe2jgvFtRRLalS3DoophuLP8VSzYq8XfmaaOtKKPis5Ms4+oA06sA35+3Logzf3yLlNVvbaY1OSiRrxN2lS91lZLYswmNWa2m3tgGdYCWexTU9/RTw5IHH5+DPjmbmDjXCviMfg8joht6wLgq6HAT/Vc3NVoTTM7zxpoz9XXT28ElgwEPu5Xv/NU2Dj/kyva/YU0WeX6Bm6uW/sssPsz6b57ACY15PGiQnzx26QBWDIqETd2iMW7FQ/gkNgGH24+jdFL95g4oh4P4oKLwPcPAodWWFfeUnJUydvP/PElV2tuk8mrXr8VB2x4weB6ZpIXU/2DqsdRF7YcX5QjDTHf/r7p40VR+uu7vs0nyiNSs4SmVHp/fI30/Z9Paj/WqM+UhX8nl0/ap0akssP4iT/qf65KNiWaZso2VHJ3dLX0XXXRYrFGITi66rVhTZu9edhUA0xqqFFQeMnw3+uj8PmYHjj6SjLG9Y/T77sqBtnvQt8Ms895Kh+4AJBz1Hy50rya2wwfMupCaSr5Sub+qjY7s7E1D0A79anZ+g5w+TiQMsf0uQ98J/WTWD7c+nOasri/1Czx9wLbj7Wm2SXjH+CjnsDHZvprXTkF7FlasxbNFL9Qm0OslS1JjbmyRvfBRVZf3/I2sPElZ0dhPz7BVa8tzXVFRpjUUKPjr/DC3KHXI/XZgXj+9vYYpn4Fa7W9cFms+iVypncd2+LLVbWXsUZZvpXXK6zZWmapOcBcTU1FmekHmCP7xJj6xW14/V1LpO9nN9cvpkqZ+6tfrPZjrOlT8+8v0ndzQ5c/7AGsmWK+k7gh39DayxixpmbFhiTE7M+vgWpq6tNaufl1YNt7gMrMcHN1sXX9plyF4c+pxv9pezYhuuIiZnXHpIYarTbNAjHhpjb46/VxKL9nKWZ7Pavfd+uW1uhatgSf992I0ub9HR+ctdXNZ/8C9n9rvM3SQ8tcUlNyxbh2SH8uB/aJMXEtjdbgswjyGvsdzp4dhc9vr72MvWpqqsed+gqw8WXbjjPabvCzsmefGnsw1aRbXgS8EQN8YGYUpDm2rs1mT0azV1e/xy42KtCFMKmhRk8uE3B39xb46IXJ2NZjESaHfQQ/bzkKEIjXNueg75nReEHzCJZX3OK4oEw1K5lyOqXmtrrU1Pz4CPBGdI3NBaX17L9i04O/5i/qghKDv6wN+wpdPgm8363mvD02MfFg0JQBS4cAf71l5hA7JjX//gL8PtlyGcMmiPowXBOq+IrU1Lft3dqTZ2uSGlcb2i6YeKwpr83PU3DB+vOkzAVejwQumup35wCunDi6MCY1RNfIZQIG3DEG709+GFumD0Kwrxe8ZALyEYTvtEl4reJhrNH2xjFdS/0xZdc/WMeLKSzvr2s/BZ22lpoa284r7vsGOFXL+j72mlHYxGkEw42GNTXrZwJ56cBvk6w/vzWO/ASc3wb89YaZGO08lHnvMsv7DRM5a65nrsznBgm50UiqWs5p9ufnoKRmx4fAL08AOluuYafmlMrFVJ3VT8fw/3FD3mN7j4ZzMiY1RCZEBPni0EvJOP3G7dg4dSDu6d4cJfDFJM1kTNM8AQAoEX3Qe+8tKBZ9bL+AueUR6uvbe01Pbvb3AmneC3M1NWaE/vUC8J0V6/tUqKXVyKsnTbWtVVXLg1pm+PA0fMDb+DlMMnVtc6PA9Mc08Dw1oigtOJpz7NoGw3tVx+vt+sz4/eVjVa9r66xstqbGYLtdaxGq/Uz+fAE4+L2N/ahqS9RsTEadNQGkxXvswEQk/W/g3euBk3867pr1wKSGqBZtIwLx7vBuOPPG7fhiTA/8K8bhzvJX8YB6NgoQiBvKP7X9pLXV1NTV2c2mh3lvelVad6euyUD1X+xG70Xgt6eAz26RmjSMytXyIDbaX/PhIYPBfsNmBW9/y+e1imj7w0LXwEnN6VRpwdHKkVOGf0WbWxKjNmunmd9nbkqBSuY+Y11WK68PdZH1ZXVaNOgs145isU+NA301VBpiv/x+58VgAyY1RFaSywTc2iES594cgqljH8Qdg29H81A/lEOBLypuq1H+nC7S/Mm86lC7U1/lqronNaY6ERuqnJdny9tSjVCl2v7KNfrFXbOscfOTwa+rk+ssn9caZ/8CXmlS7YK1JDUNXVOTVX1ElkE8Vv3sbKxVqC1RMpvUGMRi7oFbfFWaJ+mlEON/E4A0d9Jfb5o4yA41ELX9XOxRy2dPWQeBre/WHJllcfSTiT8y6pr0epgGqgMn8myD2kVgULsIPDGwDQrLNPjrRHeM2jYeA7O+QISQj626zvhb2wWtZVl4w+tzrIl9FmMzX0WQKDUNiV0fgrD9PelkiiAHreCMuv9CL8ySHhbhCTX3GSYjWrU0wmTECqDdbcDmWobG11JTEyyUSvPWdH7AuPmpoRxYbnl/gy+TUO2hLtia1NiotnNaldSYKfPbpKoO75VLdwBSZ91z19Y0u3EaILfzY6h6PIXV1ktztU63n94kfZfJgf4GHcerzwVkqV/R6v9JExc+fQAIsvDHlEl1SCSLr0hJVHDNwQXOxqSGqJ6CfL0xtGsMhnaNgapsELacuAzvs1fR7GI+TuZH4ebi94AzwG/CbHQQzuOSGI68TUHYdK2y5tLQb9H8p7tsv7B/uDQU2xZ1/YVeORR24m6g2XW1l9/2npTUmGRmzhRztTrb3wfOpwG+Iab367T2S3gu7LR83lqSsFqZm7umkqWaooZ4GNdaU2PF5Hvm4jIcsm6YBBnWSGjLqyU1VsyzUxud1rj8O+2AcQY1e65WU1Op+grqumr32OjfXrV/JwevJeP7vgIGTm+Q8PREEZh/bSmW5zMBRUDDXs9GTGqI7CjYIMEBgAqtDmOW7sL201dxSmyBU2ILfdk3NCOQLYbh5PKjWGdta1Rwi6op5ENa1CGpqecv9LObTSQ1Jh446hIg96yZGMz0x7DUv+PiLuC6wab3lasAvzDzx9aVVmM5qTH7oLWQmHw/wrYYjIb1WvGzMyxfXgT4BFouX+c+NVbU1Bg2F5p7XVFu3UNx1RggPh3wb1J72dr6n7jKDMiAtNaVXvUmpWrNT1bVDDqgA7HWICn9dzXQfWTDX9MG7FND1IC85DJ892gf7Jv9Hxx+6b+4u3tz9IwLQ8foYCzRDsWvugE4JrbEL9qaE/yJMi+IlfOUeAcA3UcBD/9UVaBJa9sDOlhL80ptrP2LuTALWNS96r2fwcPIXF+B2tZ0Mjf5XpkKOL5WWkIh+19g8xvA2uesi9MSrYnZZ+vb/JR9xLbyRvfKxoR0w/O1l7FLnxpzSY3Bz8swkTEsb+oe669R7bzVO6GbPc5E0lKvOV8acPTTtxZGFlZvfrLm31tdchpbh3Qb/pv59X91uGDDYk0NkQM0CZBGO703vJt+27EsFVbuvoD0K8X4ongWNmb9iTChEEd1rVAEP5wUYwGIECBCKAOuzwhDk6sqfFV5AlP9W2pz7Pd6fhIrl1KoXoPkGwyU5kqvK5cRqH5sbSsrm/vlW64CVlyrAfn+waomnpBYILor0LqOqw83RFJTq2qfUVePpOaEFZ2pG7JPjbnaGcP7Wv1nbmk5h9o6q5s7DjCO19X61Jijq15TY4e4tRqpU3LbW4EWPWw/ft1MoP/T9Y+jATGpIXKSDtHBeOnO6/XvD13sjKXbz2Hf/ksGpYRraQ1w+FKBtMlX+qYKbA3/yf/C6/3r4TB1nrPDTEJS+UAsVNY+Jb25fjNlButtGfZZSZktfX+pQJp5eNcS4KGV1oULmElqGngos1BtDh+ba2oMfj7FOeaLVapzTY2lkTnXmEtqDBNeS2sxVZ+awNp/e6ZiNvxZ2pocWlpQtiHVpabGUlVN5gHg8Cppgdu/3pD+X9hq5yfSlwtjUkPkIrq0CMV7w7vhveHdcDK7ED/tu4h1h5XIyDVe6PHu8pdxg+wUvvwpACIO4pxvtROFtAQKaumQWlcbZgFdHwQun7DtOHMPPlEH7FwCrLOiucjchIW1LSKqKauaeThlbu3XqWQyqRFNvzZHlVl7GVE0XQul1dSvpsYadpmnxpqkxszD1tJkh6vGVQ/GfFlDpkYJGTZtVsYrikBRNhAUZfl8pXlS0l1bufqqMRdUXfrUmHH5JLCkjjWWVp3/hNTcOXAmENuz4a5jBfapIXJB10UGYdZtHfD39Jtx7s0hOPJyMj4eeQPu6haD/WICvtDeDvHaf9/by9/A4oqh6Fz2ObqWLcEw0cy6RTZ4oHy2+Z2LbwQKrXhYGzL34NNprUtoAPO1CrX9JV1m8BepLZO4mbqeqeanQz8AGTtrlgWAdzvUfh2jZMVwsj11tY7UDZDU1HVGYVPNT4d/BBZcJ41UA8zX1BiqXlNjmPxk7KgWi7U1NSYm3zNs5qqMfeNcaWTUvq9rP+eVk1Wva7tn9VGhBnLTpdfVO9Rb1afGTPJ4cbelg6wOz6xv75M6PX+RVP9z1ROTGiI3EOjjhds7R+P9B7vj7Bu348wbt+PXif1xQ8tQpHu1wUJhJArhjwIE4kCOiHkaqY9JmeiNR/3exwavm62+1jOtfsIusQNUop/pApWjr2xhLiGx5a/PY7+Z3p76iuXjDJMaW5rPautTkzIbyDkO/PwY8OV/pXMfXFHzYVyb356qei1US2qq19TkXwA+7Flz6QN9fNU+3/laYsk/L01/b83Q7Uprp0vLF+jj0kqjYH4aL9V8rHlG2i4zeLyYazqsrKm5clpa3DPvvIVgra2pMZFAG9bUVNbkbH9f+r5uphUnNfi57PnCujjMURdL/cpqLGciAl/fBSzqBpzZXLNzs1XrX5lLUBp4qYeGqhmuAzY/EbkZmUz6xdU1NhQ//08aNVWirkBReQVW7LqAlbsvYA0ewKf5Q6UDyoFdGIHvZT2QKDuFp7xWWzx/ni4AQDlmB7+Od1TT4CXYoe+IuWaO/d9KI7s0xfW/hjkbZtVexhRTSU315Mywf8ix34BfHrf9Oge/B+5eLL02GhmkqdmnJmW2VGuwdhrQ67Haz73U3FxB1/wxVfpeOVmipkyKQeFfMx5AerDuqrYsSF46sGps1Xvva8mwudFPhiqTjWVDgCKl6TKV9n8LDL2WiFhcRNVUUmOipqbqAMvXBYyTzUt7ay9vye9TgMM/ANeZ+NlUJsT7vgICDSbRs7pPjRnWHqvTGSej1h7jQpjUEHkAf4UX/BVeePrWBDx9qzQqSkpyMpCZX4b1R7LwV0Egtus645SuBXborkch/PCK1zIM9/oLB3StcUQXj5+0N2H/KWmU0k2DkjE3zRuvX3nK0qWtY65D6F9vAL6hQEPO8G44F4gtDwZTMWtKqpUxqAE4sd62uAxVTvRnmDRp1TVH7ZTkmj4+P6N+64md2wYkJEvNR1o1MOsCIPeuWc5Uopd3zvh9yLW5mKxpfqo8X20JDSDdi5Jcaa4aW1eGN9WnxiY2LCx6ZpNU+/Wfl00vh3L4B+l79aU+qn+m6rV01TsOmwyzDk1Jhse80gQY/i3Q4Q7rj7c0LN8JmNQQeahAHy88eqM0l82coR3127W6O7FqzwXsSs/Fj3kzMO/cCKgQAF211ugmgQo88dB9uGN+Adb4vFi/YCz94ivLr9+5bVHbjL6GKq4NIdZWALs/B+JvrNknx7CTcn3mANKUShPl1UhqDB5eWo3puXzKC4GFnet+bUBKLNWFQPm1prpCJRAaW/MBburnWH3UmuLahH9W9ampZcRbdcVXriU1FhITk81PlmpqDKiygIw0oMOdxturj0qz5Ju7pe8hzYF+df2DQKg5a7O56QSsqSmxutlVBFaOtG1klK0/wwbGpIaokZHLBDzYqyUe7NUSACCKfXEqpwjf/XMeR7NU6BHXBMG+3rgpoRnkMgFvPzUGXRZFoIlQiN6y42iCQpwVo9FEUKGzkI6HvDYBAMarn8Xjt3REr23ja140qjOQuc+RH9O0y8esL/v9Q8DIVcDVU8D6GabLbJlvn7gqkxrDZjqtpmZH4QqDuVrKCoA1U4HIqoS1zrwUwIe9am6vPsrNZFJTrfZK33/GzGKkhiwN6Tal+LI0o7Wl2hKTzU+GfWqq7Td84C8ZKPULSp5X7QQWampyjkvNRk0TpMS30skNQHQ3420WGd4voeb0AeamEzD6vDb0qTE36s5WRv8mHDCjcS2Y1BA1coIg4LrIILx8VyeT+zvGBOPQm8OxPyMPhy4WYO/5PGw4KI1++h5Aqq47VGIAdovtkboRSJZNwZ3yHVgqDsVi+QL4owz/OTsGk5p0wF2xpVD3+h9Cv7FulMR+XVsclbfHQ7f0gJD6kp0+sZU0xdJfrdffbb5Mzr/2udaFncCfL0r9Uyrpqg3p3vKWtKJzpb/nA0d+BGycpNik7H+Nm4B0GqkJpXqn2D1f1jx2xyLj95UPSsMaEXvV1BRlS98tJTX7vwXC4sxfp3pSU1Fa9YCvPP/xP4zLGCUO1RKEPV9IcyABxjUc57ZKX//bCUS0Nx+vSYKJyfdMLNFRdLlqcVDAfJJiqqZGqzbdPGYrw3vrVX1+CcdjUkNEVuneMgzdW4ZhTL84LBrRHUcuFWDz8RzsTA/H7tNXEB3ii6yCMmzQ9cIGnfRXf4+Kqom6ZuUOwaxcAAdz0FuYjVne30EA0FVmeo0oleiHu9XSyKbrdIBTZr8ozTP9ILe3lSbWz9GqjR+mhgtEAsDhn2A3h6pNSlheBCy/v2a5za/Xfq7KIc86K2oQqtfy1KZyQj5LswL/+wsw4BnjbbX1qTmTCrQ1SLSrN1EZNgtWT6gqExpzsg5al9QYJh7FOdJ8U5WunACaJ1a93/cN0H6INJTa1ukVKlWUXUtqLNSuWNNsZVjb5m1mxKQDMakhojrp1DwEnZqHwLDXgCiK2Hs+D59vTUdksA9SjmYjs6DmX+M7xQ4Ypn4NgIj75VuwV3cd/iPbCwiAOqgVJpd8gPcq7tOXH75Oh09CB6OHeg+a6qpGHInthkA48UeN83sEdbHlh3ddH2bWKM2r+7GVzRHWzKuzbjqQONb6c1culVBbZ93qD2PD2oQ/pgEt+xjvzzxQLamp1nPdaM4YG4dHH/gOgChNWmmt9L+Na7e2viN9VSrIAD7pZ+JAG5qfKhO9wizThygPA1/daXqfIcMJFJnUEJEnEQQBPeKaoEectIBlZZOWukIHVZkGx7JUOHghH0pVGU5mF2Hf+Tys90pCYXkFPtVKK5sjD1iKJQAE/DqxP55ddRCnc4rweP5oyDES07x+wAldLPIRgIQrTfECpKQm/d4/oDj4LSLP/w4vjQ2T7LmqnZ/aZ70fkwRYHMpsj6SmSZuqB6alBOe1COvPXZmc1JrUVNu/d2nV6+zD0pchnRZQGrTjVY9XZ6GmxpCpifnSt0hf1w0G/EItBV3tbT2GSmsrpGbBtklAdBczI8LKpI7R1ddpq/TbU1XrtVli1Pxkh+asemJSQ0QNTuElQ3igD25MaIYbE5oZ7dPpROw6l4vi8grENvHHrvRcrD+iRLuoIHRpEYJf/tcPT3+/H5tPXIYWcrxVMUJ/7JZLOnTwHoB0XRQ++K4AwFDIcTue8voFj8vXwE9QY2/If5FY8GfDfbjQllJfAsNZZ+3h+Bogwg6dgE0Jbm55EkVrHmbmVDbVGA4Jt7Qcgi0qyq6tiVXLA9+wn4k1dBXS5IH699qa+62Kz8Kim5oSy0lNvRebRdUcOqf+BFJflr5evGw62aooB5QWZhq2duZkw8ktvVhTQ0SNnEwmoE/rpvr310UG4eE+rfTvg3y9sXRcLxSXVyC3WI3fDmaioFQDAcDS7ecwVfM/o/NpIcfCivuwsOI+hEEFVVkARIzGW4ovcb9sE8ZqZuK8LgKbfabiQvhNmJPZGzfLDmC0VwoA4Mxdv6LNr3eZD1iQGT9UvXyBSQYPh5dC6nU/jFicYbceBs0AdnxgPhGzd/OTraOczNGUAUtvr31em8z9tp1X1BovoGmpT83xNZbjM8cRq4MfXS0N8TZczPTsZtOj9yrKgCunLJzMymY2w7mTWFNDRGSdAB8vBPh4YeLNbfXbpv73OsgEAVqdiDOXi3AyuxBymQwVWh32Z+RjZ/pV5GVLTVHPqR/FixiNckiT1HUtW4Lii76ogBe26zqhl+w4CuGH+1cW4SaZ9BD4WmG8jlZp047IejAFrS/+Cvx6LZkKim64D91QMy2HtpISsZWjTC8/Ud+kJnO/cW1J9aHgzXsAl/bYfu7CLNuXobCGrgKQGdQsXT5uvL8y+Su4ZPk81SciNOSoSeoqSo2Tsr1fmS6X8Q/wV/Wh6wasbf4qNmi+aoiV623EpIaI3JaPlzQnirccuD4mBNfHVNWS3HODNLOtVidCqSrD2kNZEATAWy7Dn0eVKFWHIqugDFkFZVDDG4PVVQnM37quAIC4suXwgRrBKIECGuRfCkTxO1sAhOA5/5EYKO7CRwWj0S/tHHacuYp1R5R4OmwSppR+DBmkX/BiQASEO96T+jZc2ic1V+3/Rpomv2Vv4E2DUS6OUjlBXp//mU5q6jM78rmtwJJBxtuqNz8FRwO15AcmGdam2JOmzPz6VID08B8003yn2krfWBj+76hJ6tQlVR2qAcA32HS5f1dbPk9tC8VWMuyT0xCLrtqISQ0ReTS5TEDzUD88dlNr/bYx/eIASKO1hGtzexSWaXAyuwjKgjK8t/EkesY1wfmrxdhzPg+XK6ovQSBgfskQzMcQoBRY92vVfDWL8vphEfqihXAZfX0zsOpqIvCVAOAwkq9vgQd6NEdRi2nYvC8HudtO4LOmHeBzVZoUUNf7f5A1u05aD+qCmZW/7UERIH1v1ReYdgpYkGC8/6qlZok6qN4skziubn1Iis10aq2v8sLam4eO/Fx7GXX1RSoNmJoNuiFoiqXEplKNhTOvCbTQQVt52Py+6iw12zkBkxoiarQEg8nKgny9kdgqDAAwpEvNJqUzl4uQX6KGIAi4kFuCzPwyHMkswLkrxSgqr8D5q4Zzrgi4KEZgVanxg2PDv9nY8G+20bb2eAGPyf/APl0C9myR5jPpFf8yHm+zBZE5W7GuIA6dAwtwU+gV+Ha7H7K1U/XHapq2g/fVE0CbW4ALu6seqp3ukybmi7je9ASBoQa1Q/7h1tyq+rm4y/h9m1sAmZftD0F7TXZYXbnK/ASBlX4cV79rOCqpURcbz/9jbr2w6kt+GFo8wPrrFbOmhojI7bRpFqh/fUPLMLPl0q8UQ6sTobjWzHXoYgHySzU4lV2IpoEKXClUI6ewDAovGco0OoiQYYl2qNE5dqXnYhc6A7i2plPuta+zwEte/0V/r+OYj1HYcikB3WWnkXWpK/LUj+Ptpn/gvCIBewu6Y+LA29G212AE+PoA390ndRitVLkKN2DdqszR3YCsA7WXs5YgSOtNmRtO7GiWOv/ayx9TgZE/Nvx1MvdXS2rMNNmV5tvneobnr76KvRMIomjrTELuSaVSISQkBAUFBQgONtPGSETUwERRhChKo76Kyytw6GIBjitVaNMsEOevFmPLycv4+9QVBPp4oVyjRbFai0AfLxSV1/2v4NgmfugdCdynXIifSrtB2/EelGm0yMwvQ1ZBKZaqn0NHnEVZdE/4ZkkjuXQt+0E24BkUtrwZ3nIZfF9voj9factBONl1Orr+frvtwTx9AGgSL838u2psnT9TnQyYKs1ZU5+O0PUR3k6aHdiR/MNNJ4/h19lnGgLDzxTcAphq/9o0W57fTGqIiFxUeYUWxeVahPp5Y19GHrIKyiATBGi0OuQWq9EsyAcbj2Xj1wPS7MJdWoQgxM8bW0/ZVgPSBCoEC8W4LIZigfdibNQm4ifdTUZl5nstxv1ef0Mj98PzZaOwSjsISS1lWJT3Pwgd7oBu8FvQZR9DUEQr4Opp4Mtk6UC/MGlkUXEO0O52YMT3VSfNPgp80rde98gmt84Bdi6pfUi4u+g28tqMxXUQEGE89NseAqOAafZP2pjUmMCkhogaC1EUcbVYjX8zVQjx88aec7kQBAGiKGL3uVxczCvFsSwVdAa//SOCfJBTaLnfhwCpucyQDDroDLbJZQJEnRYzvb7HeTESaWF3Ytqt8Wh55S/sl3fFxnMahPh5Y2TvlujUPAT/XsxD95yf4b1hetVJ426UanP2fQ0AODjwM8T9+wlChNKaK62P/AnIPQuse8500IY1FePWAb9PqapZCGgmrfztrga/ZX4F+drIFQ0zzNxwUU87afCk5qOPPsL8+fOhVCrRtWtXfPDBB+jVy8Sy9desWrUKs2fPxrlz55CQkIC33noLt99eVW0piiLmzp2Lzz77DPn5+ejfvz8++eQTJCRIPfLPnTuHV199FZs2bYJSqURMTAwefvhhvPDCC1Aoqo9KMI1JDRGReWUaLf46IT3gmwYq8PfJy0g5mo28EjU0WhG5xWo0D/XT1x6Vauw7mdxI+UYMlu3CE5pnUAw/3NEpHP+R7cevlyOwKUua1G3B/V3h6y2DQleOoq0fwzeyLW67fwIAQHg51PSJez0urS919TTQ8U4g6xDw6Y3SvrZJwOmNVWUFOXDbW9Los0v7YHICuvuXOb7ZrNKoX4CfH6+qYRm7FlhWhybAhjRiBdDuNrueskGTmpUrV2L06NFYvHgxevfujYULF2LVqlU4ceIEIiJqDhHbsWMHbrrpJsybNw933HEHli9fjrfeegv79u1Dp07SujBvvfUW5s2bh6+++grx8fGYPXs2Dh8+jKNHj8LX1xfr16/HypUrMWLECLRt2xZHjhzBY489hlGjRmHBggV2vylERFS7K0XlCPTxwtEsFfy85ZDLBKw/okRGbgmUBWXYdvoKbkwIR36JBnklalzMs7CMQD31lf2L/ynWYkH53egnO4p+vuloqSjEIxWz0KVtSyS2CsOxLBU6NQ9B3r6fcWtpCpqP+RyBG6YCJ9ZKJ2naFnjq2lIDmjLgxB/Ab5OleXUq+5+8VAAc/wO6vAwI8QNw8mI2Zu70xcqSR6EormUem+oCI6XmueqT/ZnzYg4AAXjt2lIjj2yoauZztCatpRqy6jreBTzwtV0v1aBJTe/evdGzZ098+OGHAACdTofY2Fg89dRTmDlzZo3yw4cPR3FxMdasqepd3qdPH3Tr1g2LFy+GKIqIiYnBs88+i2nTpgEACgoKEBkZiWXLluHBB02vbDp//nx88sknOHvWxE01gUkNEZHzXS4sR9MABbacvAxfbznCAxXYl5EHmSBg1d6LuJhbghZN/NE2IhAnlIXYe75hO/V2aRGCHuK/uOPql5hT8QjOyeNwS/sIaHVSU12vVqEoKbyKZ/Lm4d+IIfBNfAjBfl54ZNkeRAT54EpROXQi4IcyrOi8H52vvx55On/4nf4D/ul/mu6ULPMC7v0CiL9JSmq+ux84nQL0nSQtfPnVHSZjbV3+Hb4e1xsDll+bVXt8ClCoBH4Y1YB3yIz2d5geNdZhKDD8W7teypbnt01DutVqNfbu3YtZs2bpt8lkMiQlJSEtLc3kMWlpaZg6darRtuTkZKxevRoAkJ6eDqVSiaSkqmXfQ0JC0Lt3b6SlpZlNagoKCtCkSROT+wCgvLwc5eVV7cMqlarWz0dERA2rWZDUlHRz+6qa/YTIIADA/T1iLR5bUKKBUlUGf4UcxeoKBCi8cORSAVKOZkPhJUOwnzfWH1Eiq6AUGq2I8EAFfLzkiA7xxXFlIco0WoT6K3ClqOrZcOhiAQ6hBb7EHGlDRQV+O5ip37/miDSv0CY8B6gAnD6g32fYB6kUvrjrcF9AP2/dXXg4bhCmlT+Pb/1GIvy6Pnhw/8MAgOURzyKoohfaFXrjSuZVpPpORXLPIcgIuQl9w2IQ0/kBCGdSgYm7UZq2BHu2/Ymf1H2gEwU88vUenLz25L4sj4SsZVc0fe4sML9qckn0fBS4/h5pbh1vf6DDHUDMDcDqJ4EWPW1f8NOU1oOMk5ouDwKHVkh9dZzIpqTmypUr0Gq1iIyMNNoeGRmJ48dNV58plUqT5ZVKpX5/5TZzZao7ffo0PvjgA4tNT/PmzcPLL79s+QMREZHbCPH3Roi/t9G22Cb+uK1z1WSJz9/eodbzlFdokXI0G5n5pShV6+DrLUNuiRrqCh0u5pUiyNcL/go5yjQ6HLlUgONKCzMFW/DtuRB8iw+BEgFIA17AN4gTlDhzLgY4Z7zo5heIAHD82tddSAgfjlOv/gOgy7UvibpCh3c7fA25uhDvLTqCIJ/juDexBVaVfYGPm6xA21vGYsmlOPgd88LIsXvQLNgfCi85rhSXI/i5C/D1lgO/TgIOLgfC4oG8dOCm5yBGd4MQ2RHiuhkQ4gdKs03/PkVaz+m+L4GPDPrNdrxLWum9UnAL4I73gLsXS3MQOZHbTb536dIlDB48GPfffz8ee+wxs+VmzZplVEOkUqkQG2v5rwAiIvJ8Pl5y3NElxqZjyjRa+HhJo7zOXC5CizB/CAKw5cRlRIX4IiO3BKF+Cpy7WozTOUW4XFSO8AAFNDoRW09dRqCPN64WleNMoZQMxDbxw4XcUoT5eyOvpPqkdQJOXTE/Em3RYS8A0gSQheUVWLbjHAA/jMkdB/wIANLq7ou3nDE6LsjXCyF+3vCSPYC4VuNwNFOFQCEXvoejkbWtFC3CMnEq5xHojgFfjmmF8Dt/w4nsIpzeXwJdwjfodHktFDdOxqDu7VFRkgeFzBsIi0fhI1twVSUiPty5CQ1gY1ITHh4OuVyO7Gzjab6zs7MRFRVl8pioqCiL5Su/Z2dnIzo62qhMt27djI7LzMzEzTffjH79+mHJkiUWY/Xx8YGPj/OXQSciIvfn61214GXbiCD96/9eLz3DurQIBQAMSLBu2QlRFHG5qBwRQb4AgEv5pSgqq0BusRr7MvJQoRVRqtFCo9WhS4sQ7D6Xi4SIIPy07yKuFqkhlwloFxWEk9mF1ZboMK+wrAKFZdIkjueuHZODAKBU6p6RV1I1HPvhL6qvPSYHMBRYdVb6AhCMjyHT+KLo9b9QoRPRIToYw7rF4PGBbayKpyHYlNQoFAokJiYiNTUVw4YNAyB1FE5NTcWkSZNMHtO3b1+kpqZiypQp+m0pKSno21eacCk+Ph5RUVFITU3VJzEqlQo7d+7Ek08+qT/m0qVLuPnmm5GYmIilS5dCZs3U3kRERC5IEAR9QgMAzUP99K/7tmlao/xd3aQansrFWKsTRRHlFTpk5pcizF+BbaevQC4ToJDL4CWXrvXrgUvIK1HjhpZhUKrKsP6IEoIgoHPzYOhEoHWzAKQczcb+jHyrPoMKAYAGqBz6fixLhTB/b/dJagBg6tSpGDNmDHr06IFevXph4cKFKC4uxrhx0mJfo0ePRvPmzTFv3jwAwOTJkzFw4EC88847GDJkCFasWIE9e/boa1oEQcCUKVPw2muvISEhQT+kOyYmRp84Xbp0CYMGDUKrVq2wYMECXL5cNVmSuRoiIiKixkIQBPh6y9H62hplQ7vWbF7rGGM8cmhK0nU1yvxvkDSySqsTIROk85ZptPD1lqNUrcVxpQopR7PRqqk/gny9UarWwksuQBAE5KjKEB8e0ACfzno2JzXDhw/H5cuXMWfOHCiVSnTr1g3r16/Xd/TNyMgwqkXp168fli9fjhdffBHPP/88EhISsHr1av0cNQAwffp0FBcXY8KECcjPz8eAAQOwfv16+PpKWWxKSgpOnz6N06dPo0WLFkbxNJIJkYmIiBxGLqvqH1PZ9OankKN7yzB0t7Cgq7NxmQQiIiJyWbY8v9kxhYiIiDwCkxoiIiLyCExqiIiIyCMwqSEiIiKPwKSGiIiIPAKTGiIiIvIITGqIiIjIIzCpISIiIo/ApIaIiIg8ApMaIiIi8ghMaoiIiMgjMKkhIiIij2DzKt3uqnLdTpVK5eRIiIiIyFqVz21r1t9uNElNYWEhACA2NtbJkRAREZGtCgsLERISYrGMIFqT+ngAnU6HzMxMBAUFQRAEu55bpVIhNjYWFy5cqHVZ9MaO98p6vFfW472yDe+X9XivrNdQ90oURRQWFiImJgYymeVeM42mpkYmk6FFixYNeo3g4GD+o7cS75X1eK+sx3tlG94v6/FeWa8h7lVtNTSV2FGYiIiIPAKTGiIiIvIITGrswMfHB3PnzoWPj4+zQ3F5vFfW472yHu+VbXi/rMd7ZT1XuFeNpqMwEREReTbW1BAREZFHYFJDREREHoFJDREREXkEJjVERETkEZjU1NNHH32EuLg4+Pr6onfv3ti1a5ezQ3K4efPmoWfPnggKCkJERASGDRuGEydOGJUpKyvDxIkT0bRpUwQGBuLee+9Fdna2UZmMjAwMGTIE/v7+iIiIwHPPPYeKigpHfhSHe/PNNyEIAqZMmaLfxntV5dKlS3j44YfRtGlT+Pn5oXPnztizZ49+vyiKmDNnDqKjo+Hn54ekpCScOnXK6By5ubkYOXIkgoODERoaivHjx6OoqMjRH6VBabVazJ49G/Hx8fDz80ObNm3w6quvGq2V05jv1d9//42hQ4ciJiYGgiBg9erVRvvtdW8OHTqEG2+8Eb6+voiNjcXbb7/d0B/N7izdK41GgxkzZqBz584ICAhATEwMRo8ejczMTKNzOPVeiVRnK1asEBUKhfjll1+K//77r/jYY4+JoaGhYnZ2trNDc6jk5GRx6dKl4pEjR8QDBw6It99+u9iyZUuxqKhIX+aJJ54QY2NjxdTUVHHPnj1inz59xH79+un3V1RUiJ06dRKTkpLE/fv3i2vXrhXDw8PFWbNmOeMjOcSuXbvEuLg4sUuXLuLkyZP123mvJLm5uWKrVq3EsWPHijt37hTPnj0rbtiwQTx9+rS+zJtvvimGhISIq1evFg8ePCjeeeedYnx8vFhaWqovM3jwYLFr167iP//8I27dulVs27atOGLECGd8pAbz+uuvi02bNhXXrFkjpqeni6tWrRIDAwPF999/X1+mMd+rtWvXii+88IL4888/iwDEX375xWi/Pe5NQUGBGBkZKY4cOVI8cuSI+P3334t+fn7ip59+6qiPaReW7lV+fr6YlJQkrly5Ujx+/LiYlpYm9urVS0xMTDQ6hzPvFZOaeujVq5c4ceJE/XutVivGxMSI8+bNc2JUzpeTkyMCELds2SKKovQfwdvbW1y1apW+zLFjx0QAYlpamiiK0n8kmUwmKpVKfZlPPvlEDA4OFsvLyx37ARygsLBQTEhIEFNSUsSBAwfqkxreqyozZswQBwwYYHa/TqcTo6KixPnz5+u35efniz4+PuL3338viqIoHj16VAQg7t69W19m3bp1oiAI4qVLlxoueAcbMmSI+Mgjjxhtu+eee8SRI0eKosh7Zaj6g9pe9+bjjz8Ww8LCjP4PzpgxQ2zXrl0Df6KGYyoBrG7Xrl0iAPH8+fOiKDr/XrH5qY7UajX27t2LpKQk/TaZTIakpCSkpaU5MTLnKygoAAA0adIEALB3715oNBqje9W+fXu0bNlSf6/S0tLQuXNnREZG6sskJydDpVLh33//dWD0jjFx4kQMGTLE6J4AvFeGfvvtN/To0QP3338/IiIi0L17d3z22Wf6/enp6VAqlUb3KiQkBL179za6V6GhoejRo4e+TFJSEmQyGXbu3Om4D9PA+vXrh9TUVJw8eRIAcPDgQWzbtg233XYbAN4rS+x1b9LS0nDTTTdBoVDoyyQnJ+PEiRPIy8tz0KdxvIKCAgiCgNDQUADOv1eNZkFLe7ty5Qq0Wq3RgwUAIiMjcfz4cSdF5Xw6nQ5TpkxB//790alTJwCAUqmEQqHQ/6OvFBkZCaVSqS9j6l5W7vMkK1aswL59+7B79+4a+3ivqpw9exaffPIJpk6diueffx67d+/G008/DYVCgTFjxug/q6l7YXivIiIijPZ7eXmhSZMmHnWvZs6cCZVKhfbt20Mul0Or1eL111/HyJEjAYD3ygJ73RulUon4+Pga56jcFxYW1iDxO1NZWRlmzJiBESNG6BewdPa9YlJDdjVx4kQcOXIE27Ztc3YoLunChQuYPHkyUlJS4Ovr6+xwXJpOp0OPHj3wxhtvAAC6d++OI0eOYPHixRgzZoyTo3MtP/zwA7777jssX74c119/PQ4cOIApU6YgJiaG94oahEajwQMPPABRFPHJJ584Oxw9Nj/VUXh4OORyeY1RKdnZ2YiKinJSVM41adIkrFmzBps3b0aLFi3026OioqBWq5Gfn29U3vBeRUVFmbyXlfs8xd69e5GTk4MbbrgBXl5e8PLywpYtW7Bo0SJ4eXkhMjKS9+qa6OhodOzY0Whbhw4dkJGRAaDqs1r6PxgVFYWcnByj/RUVFcjNzfWoe/Xcc89h5syZePDBB9G5c2eMGjUKzzzzDObNmweA98oSe92bxvL/EqhKaM6fP4+UlBR9LQ3g/HvFpKaOFAoFEhMTkZqaqt+m0+mQmpqKvn37OjEyxxNFEZMmTcIvv/yCTZs21ahWTExMhLe3t9G9OnHiBDIyMvT3qm/fvjh8+LDRf4bK/yzVH2zu7NZbb8Xhw4dx4MAB/VePHj0wcuRI/WveK0n//v1rTA1w8uRJtGrVCgAQHx+PqKgoo3ulUqmwc+dOo3uVn5+PvXv36sts2rQJOp0OvXv3dsCncIySkhLIZMa/zuVyOXQ6HQDeK0vsdW/69u2Lv//+GxqNRl8mJSUF7dq186imp8qE5tSpU9i4cSOaNm1qtN/p96reXY0bsRUrVog+Pj7ismXLxKNHj4oTJkwQQ0NDjUalNAZPPvmkGBISIv71119iVlaW/qukpERf5oknnhBbtmwpbtq0SdyzZ4/Yt29fsW/fvvr9lcOU//vf/4oHDhwQ169fLzZr1szjhimbYjj6SRR5ryrt2rVL9PLyEl9//XXx1KlT4nfffSf6+/uL3377rb7Mm2++KYaGhoq//vqreOjQIfGuu+4yORS3e/fu4s6dO8Vt27aJCQkJHjFM2dCYMWPE5s2b64d0//zzz2J4eLg4ffp0fZnGfK8KCwvF/fv3i/v37xcBiO+++664f/9+/Ygde9yb/Px8MTIyUhw1apR45MgRccWKFaK/v7/bDem2dK/UarV45513ii1atBAPHDhg9PvecCSTM+8Vk5p6+uCDD8SWLVuKCoVC7NWrl/jPP/84OySHA2Dya+nSpfoypaWl4v/+9z8xLCxM9Pf3F++++24xKyvL6Dznzp0Tb7vtNtHPz08MDw8Xn332WVGj0Tj40zhe9aSG96rK77//Lnbq1En08fER27dvLy5ZssRov06nE2fPni1GRkaKPj4+4q233iqeOHHCqMzVq1fFESNGiIGBgWJwcLA4btw4sbCw0JEfo8GpVCpx8uTJYsuWLUVfX1+xdevW4gsvvGD0oGnM92rz5s0mf0eNGTNGFEX73ZuDBw+KAwYMEH18fMTmzZuLb775pqM+ot1Yulfp6elmf99v3rxZfw5n3itBFA2mnCQiIiJyU+xTQ0RERB6BSQ0RERF5BCY1RERE5BGY1BAREZFHYFJDREREHoFJDREREXkEJjVERETkEZjUEBERkUdgUkNEREQegUkNEREReQQmNUREROQRmNQQERGRR/g/CEZKTxbVIa8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run_training:\n",
    "    # collect loss and add plot \n",
    "    plt.plot(np.array(trainloss)[100:,0], label = \"Training Error\")\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "    plt.plot(np.array(valloss)[100:,0], label = \"Validation Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# with open(r'C:\\Project\\smooth_autoenc\\outputs\\latent_space_verification\\trainloss_lowres_2_1000_20240726-115656', 'rb') as f:\n",
    "#     train_loss_pkl = pickle.load(f)\n",
    "\n",
    "# with open(r'C:\\Project\\smooth_autoenc\\outputs\\latent_space_verification\\vallosslowres_2_1000_20240726-115656', 'rb') as f:\n",
    "#     val_loss_pkl = pickle.load(f)\n",
    "\n",
    "# plt.plot(train_loss_pkl[100:,0], label = \"Training Error\")\n",
    "# plt.plot(val_loss_pkl[100:,0], label = \"Validation Error\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load models ###\n",
    "if load_ext:\n",
    "    trained_enc = imported_enc()\n",
    "    trained_dec = imported_dec()\n",
    "\n",
    "    trained_enc = torch.load(model_enc)\n",
    "    trained_dec = torch.load(model_dec)\n",
    "else:\n",
    "    trained_enc  = enc\n",
    "    trained_dec = dec\n",
    "\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "trained_enc.eval()\n",
    "trained_dec.eval()\n",
    "\n",
    "trained_enc_cpu = trained_enc.cpu()\n",
    "trained_dec_cpu = trained_dec.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_stats(test_data,clamp_yn, norm_yn, encoder, decorder):\n",
    "\n",
    "    X_test_all = torch.Tensor(test_data)[:,None,...]\n",
    "\n",
    "    enc_test = encoder(X_test_all)\n",
    "\n",
    "    if clamp_yn == True:\n",
    "        dec_test = decorder(enc_test).clamp(min=0, max= 1)\n",
    "    elif norm_yn == True:\n",
    "        dec_test = normalise_image(decorder(enc_test))#*mask_tensor[:730,:,:,:])\n",
    "    else:\n",
    "        dec_test = decorder(enc_test)\n",
    "         \n",
    "    X_test, dec_test = X_test_all.permute(1,0,2,3)[0], dec_test.permute(1,0,2,3)[0]\n",
    "\n",
    "    x_true, x_predict = X_test.flatten(), dec_test.flatten()\n",
    "\n",
    "    err = (x_true - x_predict)**2\n",
    "\n",
    "    mse_per_img = ((dec_test - X_test)**2).sum(dim=(1,2))/(X_test.shape[2]**2)\n",
    "\n",
    "    mae_per_img = (torch.abs(dec_test - X_test)).sum(dim=(1,2))/(X_test.shape[2]**2)\n",
    "\n",
    "    total_err =  err.sum().detach().numpy()\n",
    "    \n",
    "    avg_err = total_err/err.shape[0]\n",
    "\n",
    "    avg_mae = mae_per_img.sum()/mae_per_img.shape[0]\n",
    "\n",
    "    print(\"Total square error: {:.4f}, MSE: {:.4f} RMSE: {:.4f}\".format(total_err,avg_err, np.sqrt(avg_err)))\n",
    "\n",
    "    #iiee\n",
    "    test_iiee = iiee_calc(dec_test, X_test)\n",
    "    \n",
    "    iiee_per_img = batched_iiee(dec_test, X_test)\n",
    "\n",
    "    print(\"Total iiee: {:.4f}\".format(test_iiee))\n",
    "\n",
    "    return [err,  total_err, avg_err, np.sqrt(avg_err), mse_per_img, mae_per_img, avg_mae, test_iiee, iiee_per_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss_tuple \u001b[38;5;241m=\u001b[39m test_data_stats(\u001b[43mX1_\u001b[49m,clamp_yn,norm_yn,trained_enc_cpu, trained_dec_cpu)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# with open('../outputs/latent_space_verification/test_loss_'+ lowres   + str(dim_latent)+'_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#         pickle.dump(test_loss_tuple, output)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../outputs/final/test_loss_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m lowres   \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(dim_latent)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epochs) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X1_' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss_tuple = test_data_stats(X1_,clamp_yn,norm_yn,trained_enc_cpu, trained_dec_cpu)\n",
    "\n",
    "# with open('../outputs/latent_space_verification/test_loss_'+ lowres   + str(dim_latent)+'_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\n",
    "#         pickle.dump(test_loss_tuple, output)\n",
    "\n",
    "with open('../outputs/final/test_loss_'+ lowres   + str(dim_latent)+'_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\n",
    "        pickle.dump(test_loss_tuple, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icedmd1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
